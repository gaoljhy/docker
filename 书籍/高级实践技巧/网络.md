docker中libnetwork提供的4种驱动，每一种都有一定的局限性，假设需要运营一个数据中心的网路，有许多宿主机，每个宿主机上运行了数百个甚至上千个docker容器，使用4种网络驱动的具体情况如下

* 使用host驱动可以让容器与宿主机公用一个网络栈，看似解决了问题，可实际上并未使用network namespace的隔离，缺乏安全性。
* 使用docker默认的bridge驱动，容器没有对外IP，只能通过NAT来实现对外通信。这种方式不能解决跨主机容器间直接通信的问题，难以满足复杂场景下的需求。
* 使用overlay驱动，可以用于支持跨主机的网络通信，但必须要配合Swarm进行配置和使用才能实现跨主机的网络通信
* 使用null驱动实际上不进行任何网络设置

下面通过一些工具和额外的操作来突破docker网络原有的限制，实现一些更高级的功能，以满足实际运用中的复杂需求。

#### 1. Linux Network Namespace

1. 使用ip netns命令操作network namespace

创建一个名为nstest的network namespace

```
ip netns add nstest
```

列出系统中已存在的network namespace

```
ip netns list
```

删除一个network namespace

```
ip netns delete nstest
```

在network namespace中执行一条命令

```
ip netns exec <network namespace name> <command>
```

显示nstest namespace中的网卡信息

```
ip netns exec nstest ip addr
```

在 network namespace中启动一个shell

```
ip netns exec <network namespace name> bash
```

这样就好像进入了这个network namespace 执行命令，若要退出则输入exit即可。


2. 使用ip命令为 network namespacec配置网卡

使用ip netns add 命令创建一个network namespace后，就拥有了一个独立的网络空间，可以根据需求来配置该网络空间，如添加网卡、配置IP、设置路由规则等。

当使用ip命令创建一个network namespace时，会默认创建一个回环设备（loopback interface： lo）。该设备默认不启动，用户最好将其启动。

```
ip netns exec nstest ip link set dev lo up
```

在主机上创建两张虚拟网卡 veth-a 和 veth-b.

```
ip link add veth-a type veth peer name veth-b
```

将veth-b设备添加到nstest这个network namespace中，veth-a留在主机中

```
ip link set veth-b netns nstest
```

现在nstest这个network namespace就有了两块网卡lo和veth-b

```
ip netns exec nstest ip link
```

可以为网卡分配ip并启动网卡了

在主机上为veth-a配置ip并启动
```
ip addr add 10.0.0.1/24 dev veth-a
ip link set dev veth-a up
```

为nstest中的veth-b配置ip 并启动

```
ip netns exec nstest ip addr add 10.0.0.2/24 dev veth-b
ip netns exec nstest ip link set dev veth-b up
```

给两张网卡配置了ip后，会爱格子的network namespace中生成一条路由，用ip route或route -a命令查看一下：

在主机中

```
ip route 
```
在nstest network namespace中

```
ip netns exec nstest ip route 
```
两条路由表明目的地址为10.0.0.0/24网络的ip包分配从veth-a和veth-b发出

测试连通性

从主机的veth-a网络ping nstest network namespace的veth-b网卡

```
ping 10.0.0.2
```

从nstest network namespace的veth-b网卡ping主机的veth-a网卡

```
ip netns exec nstest ping 10.0.0.1
```

3.将两个network namespace连接起来

有时需要搭建一个复杂的网络环境来测试数据，但受困于没有足够的资源来创建虚拟机。掌握了network namespace，可以在一台普通的机器上，创建多个相互隔离的network namespace，然后通过网卡、网桥等虚拟设备将它们连接起来，组成网络拓扑

将两个network namespace通过veth pair设备连接起来，过程如下

创建两个network namespace ns1，ns2

```
ip netns add ns1
ip netns add ns2
```

创建veth pair设备veth-a 、veth-b
```
ip link add veth-a type veth peer name veth-b
```

将网卡分别放到两个namespace中
```
ip link set veth-a netns ns1
ip link set veth-b netns ns2
```

启动两块网卡
```
ip netns exec ns1 ip link set dev veth-a up
ip netns exec ns2 ip link set dev veth-b up
```

分配ip
```
ip netns exec ns1 ip addr add 10.0.0.1/24 dev veth-a
ip netns exec ns2 ip addr add 10.0.0.2/24 dev veth-b
```
测试连通
```
ip netns exec ns1 ping 10.0.0.2
```

通过 veth pair设备连接起来的两个network namespace就好像直接通过网线连接起来的两台机器


如果有更多network namespace需要连接，就需要引入虚拟网桥了。

4. 使用ip命令配置docker容器网络

当使用默认网络模式（bridge模式）启动一个docker容器时，一定是在主机上新创建一个linux network namespace。用户可以按照在network namespace中配置网络的方法来配置docker容器的网路。

首先启动一个名为test1的docker容器

```
docker run --itd --name test1 centos /bin/bash
```

然后，使用 ip netns list 命令查看是否可以看到新创建的network namespace。执行命令后发现并没有看到新增加的network namespace。这并不代表docker容器没有创建network namespace，只是 ip netns命令无法查看，与ip netns命令的工作方式有关。

当使用ip netns命令创建了两个network namespace（ns1和ns2）后，会在/var/run/netns目录下看到ns1和ns2两项。

```
ls -la /var/run/netns/
```

ip netns list 命令在/var/run/netns目录下查找 network namespace。由于docker创建的network namespace并不在此目录下创建任何项，因此，需要一些额外的操作来使ip命令可以操纵docker创建的network namespace


linux下每一个进程都会属于一个特定的network namespace，来看一下不同network namespace环境中/proc/$PID/ns目录下有何区别


/proc/self链接到当前正在运行的进程

主机默认的network namespace中
```
ls -la /proc/self/ns
```

在ns1中
```
ip netns exec ns1 ls -la /proc/self/ns/
```

在ns2中

```
ip netns exec ns2 ls -la /proc/self/ns/
```

可以发现，不同network namespace中的进程有不同的net:[]号码分配，这些号码代表着不同的network namespace，拥有相同net:[]号码的进程属于同一个network namespace。只要将代表docker创建的network namespace的文件链接到/var/run/netns目录下，就可以使用ip netns命令进行操作了。

用docker inspect 查看test1容器的PID

```
docker inspect --format '{{ .State.Pid }}' test1
```

若不存在 /var/run/netns目录，则创建

```
mkdir -p /var/run/netns
```

在/var/run/netns目录下创建软链接，指向test1容器的network namespace

```
ln -s /proc/31203/ns/net /var/run/netns/test1
```

测试是否成功

```
ip netns list

ip netns exec test1 ip link
```

完成以上配置后，就可以自行配置docker的网络环境了。另外，在不开特权模式的情况下(--privileged=false),是没有权限直接在docker容器内部进行网络配置的，而特权模式会给主机带来安全隐患，因此最好使用ip netns exec 命令来进行docker容器网络的配置。

除了ip netns命令外，还有一些其他工具可以进行linux namespace，比如 nsenter。


#### 2. pipework 原理解析

docker现有的网络模式比较简单，扩展性和灵活性都不能满足很多复杂应用场景的需求。很多时候用户都需要自定义docker容器的网络，而非使用docker默认创建的IP和NAT规则。

1. 将docker容器配置到本地网络环境中

如果要将docker容器和容器主机处于同一网络，那么容器与主机应该处于一个二层网络中。就像把两台机器连在同一个交换机上。在虚拟场景下，虚拟网桥可以将容器连在一个二层网络中，主要将主机的网卡桥接到虚拟网桥中，将容器和主机的网络连接起来，再给docker容器分配一个本地局域网IP。


通过一个例子来分析。本地网络为 10.10.103.0/24， 网关为 10.10.103.254， 有一台IP地址为 10.10.13.91/24的主机（网卡为eth0），要在这台主机上启动一个名为test1的docker容器，，并给它配置ip为10.10.103.95/24。由于并不需要docker提供的网络，所以使用 --net=none参数来启动容器

启动一个名为test1的docker容器

```
docker run -tid --name test1 --net=none centos /bin/bash
```

创建一个供容器连接的网桥br0

```
brctl addbr br0
ip link set br0 up
```

将主机eth0桥接到br0上，并把eth0的ip配置在br0上。由于是远程操作，会导致网络断开，因此这里放在一条命令中执行。

```
ip addr add 10.10.103.91/24 dev br0 ; ip addr del 10.10.103.91/24 dev eth0 ; brctl addif br0 eth0 ; ip route del default ; ip route add default via 10.10.103.254 dev br0
```

找到test1的pid，，保存到pid中

```
pid=$(docker inspect --format '{{ .State.Pid }}' test1)
```
将容器的network namespace添加到/var/run/netns目录下

```
mkdir -p /var/run/netns
ln -s /proc/$pid/ns/net /var/run/netns/$pid
```

创建用于连接网桥和docker容器的网卡设备
将veth-a连接到br0网桥中
```
ip link add veth-a type veth peer name veth-b
brctl addif br0 veth-a
ip link set veth-a up
```

将veth-b放到test1的network namespace中，重命名为eth0 ，并为其配置ip和默认路由
```
ip link set veth-b netns $pid
ip netns exec $pid ip link set dev veth-b name eth0
ip netns exec $pid ip link set eth0 up
ip netns exec $pid ip addr add 10.10.103.95/24 dev eth0
ip netns exec $pid ip route add default via 10.10.103.254
```

完成上述配置后，docker容器与主机连接的网络拓扑。

现在test1容器可以实现与本地主机相互访问，并且test1 容器可以通过本地网络的网关10.10.103.254访问外部网络。

如果需要经常自定义docker网络可以考虑编写成shell脚本

2. pipework解析

pipework号称是容器的SDN解决方案，可以在复杂场景下将容器连接起来。它既支持普通的LXC容器，也支持docker容器。未来pipework工具的很多功能可能会被docker原生支持。

● 支持linux网桥连接容器并配置容器ip地址

下载pipework
```
git clone https://github.com/jpetazzo/pipework
```

将pipework脚本放入PATH环境变量所指定的目录下，如/usr/local/bin
```
cp ~/pipework/pipework /usr/local/bin/
```

完成test1的配置
```
pipework br0 test1 10.10.103.95/24@10.10.103.254
```

这一行配置命令执行的操作如下：

❑ 查看主机中是否存在br0网桥，不存在就创建

❑ 向test1中加入一块名为eth1的网卡，并配置ip地址为 10.10.103.95/24

❑ 若test1中已经有默认路由，则删掉，把10.10.103.254设为默认路由的网关

❑ 将test1容器连接到之前创建的网桥br0上


● 支持使用macvlan设备将容器连接到本地网络

除了使用linux bridge，还有使用主机网卡的macvlan子设备将docker容器桥连接到本地网络中。macvlan设备是从网卡上虚拟出的一块新网卡，它和主网卡分别有不同的MAC地址，可以配置独立的IP地址。macvlan早前在LXC中被广泛使用，目前docker网络本身不提供macvlan支持，，但可以通过pipework来完成macvlan配置。采用macvlan只需要执行一条命令


```
pipework eth0 test1 10.10.103.95/24@10.10.103.254
```
pipework的第一个参数是主机上的一块以太网卡，而非网桥。pipework不会再创建veth pair设备来连接容器和网桥，转而采用macvlan设备作为test1容器的网卡。操作如下：

(1) 从主机eth0上创建一块macvlan设备，将macvlan设备放入到test1中并命名为eth1

(2) 为test1中新添加的网卡配置ip地址为 10.10.103.95/24

(3) 若test1中已经有默认路由，则删掉，把10.10.103.254设为默认路由的网关


从eth0 上创建出的macvlan设备放在test1后，test1容器就可以和本地网络中的其他主机通信了。但是，如果在test1所在主机上却不能访问test1，因为进出macvlan设备的流量被主网卡eth0隔离了，主机不能通过eht0 访问macvlan设备。要解决这个问题，需要在eth0上再创建一个macvlan设备，将eth0的ip地址移到这个macvlan设备上

```
ip addr del 10.10.103.91/24 dev eth0
ip link add link eth0 dev eth0m type macvlan mode bridge
ip link set eth0m up
ip addr add 10.10.103.91/24 dev eth0m
route add default gw 10.10.103.254
```

● 支持DHCP获取容器的IP

docker容器通过DHCP方式获取IP地址，将pipework指令中的IP地址参数替换为dhcp


手动配置ip地址的命令
```
pipework eth0 test1 10.10.103.95/24@10.10.103.254
```

通过主机网络中的DHCP服务器获取IP地址的命令

```
pipework eth0 test1 dhcp
```

docker主机上需要安装有DHCP客户端（dhcpclient）。pipework根据不同的dhcp客户端执行不同的命令发送dhcp请求


● 支持Open vSwitch

Open vSwitch是一个开源的虚拟交换机，相比于linux bridge，Open vSwitch支持vlan、Qos等功能，同时还提供对OpenFlow协议的支持，可以很好地与SDN体系融合。提供对Open vSwitch的支持，有助于借助Open vSwitch的强大功能来扩展docker网络。目前pipework对Open vSwitch的支持低相对简单，并没有设计高级功能。

用法是将pipework第一个参数设为Open vSwitch网桥，若需要pipework创建Open vSwitch网桥，则要将网桥的名称以“ovs”开头。

```
pipework ovsbr0 $CONTAINERID 192.168.1.2/24
```

过程和linux bridge 基本一样，只是创建网桥和将容器连接至网桥的命令稍有差异

创建Open vSwitch网桥的命令

```
ovs-vsctl add-br $IFNAME
```

将veth pair的另一端 $LOCAL_IFNAME 放入Open vSwitch的命令
```
ovs-vsctl add-port $IFNAME $LOCAL_IFNAME ${VLAN:+"tag=$VLAN"}
```

ovs-vsctl命令是Open vSwitch操作网桥的命令，通过 ovs-vsctl --help查看使用方法

● 支持设置网卡mac地址以及配置容器vlan

pipework除了支持给网卡配置ip外，还可以指定网卡的mac地址。用法是在ip参数后面加上一个mac地址的参数。
```
pipework br0 $CONTAINERID dhcp fa:de:b0:99:52:1c
```

实现的过程用ip命令配置即可
```
[ "$MACADDR" ] && ip netns exec $NSPID ip link set dev $CONTAINER_IFNAME address $MACADDR
```
如果给docker容器划分vlan，可以把mac参数写成 [MAC]@VID, 其中 MAC地址可以省略，VID为vlan的id号。设置vlan只支持Open vSwitch和macvlan设备，不支持普通的linux网桥。

```
pipework ovsbr0  $CONTAINERID dhcp @10
```

实现过程就是在给Open vSwitch添加端口的时候指定端口的vlan id
```
ovs-vsctl add-port $IFNAME $LOCAL_IFNAME ${VLAN:+"tag=$VLAN"}
```

#### 3. pipework 跨主机通信

在大规模集群环境中，会遇到docker容器跨主机通信的问题。在目前docker默认网络环境下，单台主机上的docker容器可以通过docker0网桥直接通信，而不同主机上的docker容器之间只能通过在主机上做端口映射的方法进行通信。这种端口映射方式对很多集群应用来说极为不便。如果能使docker容器之间直接使用本身ip地址进行通信？

1.桥接

使用虚拟网桥将docker容器桥接到本地网络环境中，按照这种方法，把同一个局域网中不同主机上的docker容器都配置在主机网络环境中，它们之间可以直接通信，但这么做会出现以下问题：

❑ docker容器占用主机网络的ip地址

❑ 大量docker容器可能引起广播风暴，导致主机所在网络性能的下降

❑ docker容器连在主机网络中可能引起安全问题

如果情况不是无法回避，必须将docker容器连接在主机网络中，最好还是将其分离开。为了隔离docker容器间网络和主机网络，需要额外使用一块网卡桥接docker容器。与采用一块网络时一样：在所有主机上用虚拟网桥将本机的docker容器连接起来，然后将一块网卡加入到虚拟网桥中，使所有主机上的虚拟网桥级联在一起，这样，不同主机上的docker 容器也就如同连在了一个大的逻辑交换机上。

由于不同机器上的docker容器可能获得相同的ip地址，因此需要解决ip的冲突问题。一种方法是使用pipework为每一个容器分配一个不同的ip，而不使用docker daemon分配的ip。此方法相当烦琐，另一种方法是为每一台主机上的docker daemon指定不同的 --fixed-cidr参数，将不同主机上的docker容器的地址限定在不同的网段中。


上图，两台centos的主机host1和host2，每台主机上有两块网卡eth0和eth1。 eth0作为主机的主网卡连在主机的局域网环境中，其ip分别为 10.10.103.91/24和10.10.103.92/24；eth1用来桥接不同主机上的docker容器，因此eth1 不需要配置ip。


docker 安装完成后，在host1主机上看到docker0的ip为 172.17.42.1/16，docker容器也就是从docker0所在的网络中获取ip。在本例中，将host1上的docker容器的ip范围限制在172.17.1.0/24网段中，将host2上的docker容器的ip范围限制在172.17.2.0/24网段中，同时将host2的docker0网络桥地址改为 172.17.42.2/16，以避免和host1的docker0的IP冲突，然后将eth1桥接到docker0中，配置如下


在host1上配置
```
echo 'DOCKER_OPTS="--fixed-cidr=172.17.1.1/24"' >> /etc/default/docker
service docker stop
service docker start
```
将eth1网卡接入到docker0网桥中

```
brctl addif docker0 eth1
```

在host2上配置
```
echo 'DOCKER_OPTS="--fixed-cidr=172.17.2.1/24"' >> /etc/default/docker
```
为避免和host1上的docker0的ip冲突，修改docker0的ip

```
ifconfig docker0 172.17.42.2/16
service docker stop
service docker start
brctl addif docker0 eth1
```

对docker0网桥的上述配置只是暂时生效，在重启机器后，配置会失效。如果要持久化配置，可将docker0配置信息写入/etc/network/interfaces目录下。

host2上docker0的配置，主机网桥的配置和普通网卡的配置略有不同
```
auto docker0
iface docker0 inet static
    address 172.17.42.2
    netmask 255.255.0.0
    bridge_ports eth1
    bridge_stp off
    bridge_fd 0
```

在host1和host2上分别创建两个docker容器con1、con2，使用nc命令测试con1和con2的连接。

在host1上启动一个容器con1
```
docker run -it --rm --name con1 centos /bin/bash
```

在con1容器中执行
```
ifconfig eth0
route -n
nc -l 172.17.1.1 9000
```

启动一个容器con2
```
docker run -it --rm --name con2 centos /bin/bash
```

在con2容器中执行
```
ifconfig eth0
nc -w 1 -v 172.17.1.1 9000
```
容器con1和con2已经可以成功通信了。

容器con1（172.17.1.1)向容器con2（172.17.2.1)发送数据的过程是这样的：首先，通过查看本身的路由表发现目的地址和自己处于同一网段，那么就不需要将数据发往网关，可以直接发给con2，con1通过arp广播获取到con2的mac地址；然后，构造以太网帧发往con2即可。此过程数据流经的路径是两个容器的eth0网卡所连接的路径，其中docker0网桥充当普通的交换机转发数据帧。

2. 直接路由

另一种跨主机通信的方式是通过在主机中添加静态路由实现的。如果有两台主机host1和host2，两主机上的docker容器是两个独立的二层网络，将con1发往con2的数据流先转发到主机host2上，再由host2再转发到其上的docker容器中。反之亦然。

由于使用容器的ip进行路由，就需要避免不同主机上的docker容器使用相同的ip，所以应该为不同的主机分配不同的ip子网。

下图，两台centos的主机host1和host2，每台主机上有一块网卡。host1的ip地址为10.10.103.91/24,host2的ip地址为 10.10.103.92/24. host1上的docker容器在172.17.1.0/24子网中，host2上的docker容器在172.17.2.0/24子网中，并且在两台主机上有这样的规则--所有目的地址为172.17.1.0/24的包都被转发到host1，目的地址为172.17.2.0/24的包都被转发到host2，配置如下

在host1上配置
配置docker0
```
ifconfig docker0 172.17.1.254/24
service docker restart
```
添加路由，将目的地址为172.17.2.0/24的包转发到host2

```
route add -net 172.17.2.0 netmask 255.255.255.0 gw 10.10.103.92
```
配置iptables规则

```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE
```
启动容器con1

```
docker run  -it --name con1 centos /bin/bash
```

在con1容器中执行
```
nc -l 9000
```

在host2配置

配置docker0
```
ifconfig　docker0 172.17.2.254/24
service docker restart
```

添加路由，将目的地址为 172.17.1.0/24的包转发到host1
```
route add -net 172.17.1.0 netmask 255.255.255.0 gw 10.10.103.91
```

配置iptables规则
```
iptables -t nat -F POSTROUTING
iptables -t nat -A POSTROUTING -s 172.17.2.0/24 ! -d 172.17.0.0/16 -j MASQUERADE

```
启动容器 con2

```
docker run  -it --name con2 centos /bin/bash
```
在con2容器中执行

```
nc -w 1 -v 172.17.1.1 9000
```

需要注意的是，此处配置容器ip范围的方法和之前桥接网络中使用的方法不同。在桥接网络中，所有主机上的容器都在172.17.0.0/16这个大网络中，这从docker0的ip(172.17.42.1/16)可以看出，只是使用--fixed-cidr参数将不同主机的容器限制在这个ip网段的一个小范围内。而在直接路由方法中，不同主机上的docker容器不在同一个网络中，它们有不同的网络号，如果将host1上的docker0的ip设为172.17.1.254/24，那么host1 的docker容器就只能从172.17.1.0/24网络中获取ip。所以尽管这两种方法都使用了相同的ip地址范围，但它们的网络号是不同的，因此涉及的转发机制也不相同，桥接网络是二层通信，通过mac地址转发；直接路由为三层通信，通过ip地址进行路由转发。

上例中，在主机上添加了相应的路由之后，两个容器之间就可以通信了。启动docker daemon时会创建如下的iptables规则，用于容器与外界通信

```
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
```
从con1发往con2的包，在主机eth0转发出去时，这条规则会将包的源地址改为eth0地址（10.10.103.91），因此con2看到的包是从 10.10.103.91上发过来的。反过来从con2发往con1的包也是相同的原理。尽管这并不影响它们之间的通信，但两个容器并没有真正“看到”对方。所以上例将这条iptables规则删除了（iptables -t nat -F POSTROUTING 表示清空nat表，，也可以使用-D参数单独删除这条规则），这样两个容器之间的通信就没有snat转换了。但是删除这条规则后，容器通往外界的流量也没有了snat转换，会导致容器访问不了外部网络。为此，需要额外添加一条新的MASQUERADE规则到POSTROUTING链中，使所有目标地址不是172.17.0.0/16的包都经过SNAT转换。


综上所述，从con1发往con2 的包，首先发往con1的网关 docker0（172.17.1.254），然后通过查看主机的路由得知需要将包发给host2（10.10.103.92），包到达host2后再转发给host2的docker0（172.17.2.254），最后到达容器con2中。

这两种跨主机通信方式简单有效，但是它们要求主机在同一个局域网中。如果两台主机在不同的二层网络中，需要使用隧道技术解决容器的跨网络通信。


#### 4. OVS划分VLAN

在计算机网络中，传统的交换机虽然能隔离冲突域，提高每一个端口的性能，但并不能隔离广播域，当网络中的机器足够多时会引发广播风暴。同时，不同部门、不同组织的机器连在同一个二层网络中也会造成安全问题。因此，在交换机中划分子网、隔离广播域的思路便形成了 VLAN 的概念。VLAN  (Virtual Local Area Network）即虚拟局域网，按照功能、部门等因素将网络中的机器进行划分，使之分属于不同的部分，每一个部分形成一个虚拟的局域网络，共享一个单独的广播域。这样就可以把一个大型交换网络划分为许多个独立的广播域，即 VLAN。

VLAN 技术将一个二层网络中的机器隔离开来，那么如何区分不同 VLAN 的流量呢？IEEE802.1q 协议规定了 VLAN 的实现方法，即在传统的以太网帧中再添加一个 VLAN tag 字段，用于标识不同的 VLAN。这样，支持 VLAN 的交换机在转发帧时，不仅会关注 MAC 地址，还会考虑到 VLAN tag 字段。VLAN tag 中包含了 TPID、PCP、CFI、VID，其中 VID  (VLANID）部分用来具体指出帧是属于哪个 VLAN 的。VID 占 12 位，所以其取值范围为 0 到 4095。图 4- 5 演示了一个多交换机下 VLAN 划分的例子。

在分析图 4-5 之前，先来介绍一下交换机的 access 端口和 trunk 端口。图中，Port1、Port2、Port5、Port6、Port7、Port8 为 access 端口，每一个 access 端口都会分配一个 VLAN ID，标识它所连接的设备属于哪一个 VLAN。当数据帧从外界通过 access 端口进入交换机时，数据帧原本是不带 tag 的，access 端口给数据帧打上 tag (VLAN ID 即为 access 端口所分配的 VLANID）；当数据帧从交换机内部通过 access 端口发送时，数据帧的 VLAN ID 必须和 access 端口的 VLAN ID 一致，access 端口才接收此帧，接着 access 端口将帧的 tag 信息去掉，再发送出去。Port3、Port4 为 trunk 端口，trunk 端口不属于某个特定的 VLAN，而是交换机和交换机之间多个 VLAN 的通道。trunk 端口声明了一组 VLAN ID，表明只允许带有这些 VLAN ID 的数据帧通过，从 trunk 端口进入和出去的数据帧都是带 tag 的（不考虑默认 VLAN 的情况）。PC1 和 PC3 属于 VLAN100, PC2 和 PC4 属于 VLAN200, 所以 PC1 和 PC3 处在同一个二层网络中，PC2 和 PC4 处在同一个二层网络中。尽管 PC1 和 PC2 连接在同一台交换机中，但它们之间的通信是需要经过路由器的。


在这个例子中，VLAN tag 是如何发挥作用的呢？当 PC1 向 PC3 发送数据时，PC1 将 IP 包封装在以太帧中，帧的目的 MAC 地址为 PC3 的地址，此时帧并没有 tag 信息。当帧到达 Port1 时，Port1 给帧打上 tag  (VID=100），帧进人 switch1, 然后帧通过 Port3、Port4 到达 Switch2  (Port3、Port4 允许 VLAN ID 为 100、200 的帧通过）。在 switch2 中，Port5 所标记的 VID 和帧相同，MAC 地址也匹配，帧就发送到 Port5 上，Port5 将帧的 tag 信息去掉，然后发给 PC3。由于 PC2、PC4 与 PC1 的 VLAN 不同，因此收不到 PC1 发出的帧。

在多租户的云环境中，VLAN 是一个最基本的隔离手段。作为云计算的新宠儿一-Docker，如何实现 VLAN 的划分呢？下面详细介绍如何使用 pipework 实现 Docker 容器的 VLAN 划分。

1. 单主机 Docker 容器的 VLAN 划分

在 Docker 默认网络模式下，所有的容器都连在 docker0 网桥上。dockero 网桥是普通的 Linux 网桥，不支持 VLAN 功能，为了方便操作，使用 Open vSwitch 代替 docker0 进行 VLAN 划分。图 4-6 是一个在一台主机上进行 Docker 容器 VLAN 划分的例子。

为了演示隔离效果，图中 4 个容器都在同一个 IP 网段中，但实际上它们是二层隔离的两个网络，有不同的广播域。为完成如图 4-6 所示的配置，我们在主机 A 上做如下操作。

在主机 A 上创建 4 个 Docker 容器：con1、con2、con3、con4

```
docker run -itd --name con1 centos /bin/bash
docker run -itd --name con2 centos /bin/bash
docker run -itd --name con3 centos /bin/bash
docker run -itd --name con4 centos /bin/bash
```
使用 pipework 将 con1、con2 划分到一个 VLAN 中
```
pipework ovs0 con1 192.168.0.1/24 @100
pipework ovs0 con2 192.168.0.2/24 @100
```
使用 pipework 将 con3、con4 划分到一个 VLAN 中
```
pipework 0 vs0 con3 192.168.0.3/24 @200
pipework ovs0 con4 192.168.0.4/24 @200
```
pipework 配置完成后，每个容器都多了一块 eth1 网卡，eth1 连在 ovs0 网桥上，并且进行了 VLAN 的隔离。和之前一样，通过 nc 命令测试各容器之间的连通性时发现，con1 和 con2 可以相互通信，但与 con3 和 con4 隔离。如此一来，一个简单的 VLAN 隔离容器网络就完成了。

使用 Open vSwitch 配置 VLAN 比较简单，如创建 access 端口和 trunk 端口使用如下命令：

在 ovsO 网桥上增加两个端口 port1、port2
```
ovs-vsctl add-port ovs0 port1 tag=100
ovs-vsctl add-port ovs0 port2 trunk=100, 200
```
pipework 就是使用这样的方式将 veth pair 的一端加入到 ovs0 网桥的，只不过并不需要用到 trunk 端口。在向 Open vSwitch 中添加端口时，若不添加任何限制，此端口则转发所有帧。

2. 多主机 Docker 容器的 VLAN 划分

介绍完单主机上 VLAN 的隔离，将进-步讲解多主机的情况。多主机 VLAN 的情况下，肯定有属于同一 VLAN 但又在不同主机上的容器，因此多主机 VLAN 划分的前提是跨主机通信。在 4.2.3 节中介绍了两种跨主机通信的方式，要使不同主机上的容器处于同一 VLAN，就只能采用桥接方式。首先用桥接的方式将所有容器连接在一个逻辑交换机上，再根据具体情况进行 VLAN 的划分。桥接需要将主机的一块网卡桥接到容器所连接的 Open vSwitch 网桥上，如 4.2.3 节所述，使用一块额外的网卡 eth1 来完成”，桥接的网卡需要开启混杂模式。图 4-7 演示了一个多主机 Docker 容器 VLAN 划分的例子”。

这里，我们将不同 VLAN 的容器的设在同一个子网中，仅仅是为了演示隔离效果。图 4-7 中，host1。上的 con1 和 host2 上的 con3 属于 VLAN100, con2 和 con4 属于 VLAN200。由于会有 VLAN ID 为 100 和 VLAN ID 为 200 的帧通过，物理交换机上连接 host1 和 host2 的端口应设置为 trunk 端口。host1 和 host2 上 eth1 没有设置 VLAN 的限制，是允许所有帧通过的。完成图 4-7 所示例子需要做如下操作。

在 host1 上
```
docker run -itd --name con1 centos /bin/bash
docker run -itd --name con2 centos /bin/bash

pipework ovsO con1 192.168.0.1/24 @100

pipework ovs0 con2 192.168.0.2/24 @200

ovs-vsctl add-port ovSO eth1;
```
在 host2 上
```
docker run -itd --name con3 centos /bin/bash

docker run -itd --name con4 centos /bin/bash

pipework ovs0 con3 192.168.0.3/24 @100

pipework ovs0 con4 192.168.0.4/24 @200

ovs-vsctl add-port ovs0 eth1;
```
完成之后，再通过 nc 命令测试实验效果即可

#### 5. OVS隧道模式

跨主机通信的两种方法有一个局限一要求主机在同一个子网中。当基础设施的规模足够大时，这种局限性就会暴露出来，比如两个数据中心的 Docker 容器需要通信时，这两种方法就会失效。当前在 Docker 中主流隔离技术 VLAN ，然而 VLAN 也有诸多限制。首先，VLAN 是在二层帧头上做文章，也要求主机在同一个子网中。其次，提到过 VLANID 只有 12 个比特单位，即可用的数量为 4000 个左右，这样的数量对于公有云或大型虚拟化环境而言捉襟见肘。除此之外，VLAN 配置比较烦琐且不够灵活。这些问题就是当前云计算所面临的网络考验，目前比较普遍的解决方法是使用 Overlay 的虚拟化网络技术。

1. Overlay 技术模型

Overlay 网络其实就是隧道技术，即将一种网络协议包装在另一种协议中传输的技术。如果有两个使用 IPv6 的站点之间需要通信，而它们之间的网络使用 IPv4 协议，这时就需要将 IPv6 的数据包装在 IPv4 数据包中进行传输。隧道被广泛用于连接因使用不同网络而被隔离的主机和网络，使用隧道技术搭建的网络就是所谓的 Overlay 网络。它能有效地覆盖在基础网络之上，该模型可以很好地解决跨网络 Docker 容器实现二层通信的需求。

在普通的网络传输中，源 IP 地址和目的 IP 地址是不变的，而二层的帧头在每个路由器节点上 都会改变，这是TCP/IP协议所作的规定。那么，如何使两个中间隔离了因特网的主机像连在同一台交换机上一样通信呢？如果将以太网帧封装在 IP 包中，通过中间的因特网，最后传输到目的网络中再解封装，这样就可以保证二层帧头在传输过程中不改变，这也就是早期 Ethernet in IP 的二层 Overlay 技术。至于多租户隔离问题，解决思路是将不同租户的流量放在不同的隧道中进行隔离。用于封装传输数据的协议也会有一个类似 VLAN ID 的标识，以区分不同的隧道。图 4-8 演示了多租户环境下 Overlay 技术的应用。

当前主要的 Overlay 技术有 VXLAN (Virtual Extensible LAN）和 NVGRE (Network Virtualization using Generic Routing Encapsulation）。VXLAN 是将以太网报文封装在 UDP 传输层上的一种隧道转发模式，它采用 24 位比特标识二层网络分段，称为 VNI  (VXLAN Network Identifier），类似于 VLAN ID 的作用。NVGRE 同 VXLAN 类似，它使用 GRE 的方法来打通二层与三层之间的通路，采用 24 位比特的 GRE key 来作为网络标识（TNI）。本节主要使用 NVGRE 来演示 Docker 容器的跨网络通信。



2. GRE 简介

NVGRE 使用 GRE 协议来封装需要传送的数据，因此需要先了解-下 GRE。GRE 协议可以用来封装任何其他网络层的协议。为方便理解，这里直接通过一个 VPN 的例子来演示 GRE 封装过程。如图 4-9 所示，一个公司有两个处在不同城市的办公地点需要通信。两个地点的主机都处在 NAT 转换之下，因此两地的主机并不能直接进行 ping 或 ssh 操作。如何才能使两个办公地点相互通信呢？通过在双方路由器上配置 GRE 隧道就可实现该目的。

首先在路由器上配置一个 GRE 隧道的网卡设备。

添加一条静态路由，将目的地址为192.168.x.0/24的包通过上面配置的隧道设备发送出去。


配置完成后，分析一下从IP地址为192.168.1.1/24的主机A ping IP地址为192.168.2.1/24的主机 B 的过程。主机 A 构造好 IP 包后，通过查看路由表发现目的地址和本身不在同一个子网中，要将其转发到默认网关 192.168.1.254 上。主机 A 将 IP 包封装在以太网帧中，源 MAC 地址为本身网卡的 MAC 地址，目的 MAC 地址为网关的 MAC 地址，数据格式如图 4-10 所示。网关路由器收到数据帧后，去掉帧头，将 IP 包取出来，匹配目的 IP 地址和自身的路由表，确定包需要从 GRE 隧道设备发出，这就对这个 IP 包做 GRE 封装，即加上 GRE 协议头部。封装完成后，该包是不能直接发往互联网的，需要生成新的 IP 包作为载体来运输 GRE 数据包，新 IP 包的源地址为 1.1.1.1，目的地址为 2.2.2.2。当然，这个 IP 包会装在新的广域网二层帧中发出去，数据格式如图 4-11 所示。在传输过程中，中间的节点仅能看到最外层的 IP 包。当 IP 包到达 2.2.2.2 的路由器后，路由器将外层 IP 头部和 GRE 头部去掉，得到原始的 IP 数据包，再将其发往 192.168.2.1。对于原始 IP 包而言，两个路由器之间的传输过程就如同单条链路上的一跳。在这个例子中，GRE 协议封装的是 IP 包，实现了一个 VPN 的功能。



3. GRE 实现 Docker 容器跨网络通信（容器在同一子网中）

既然 GRE 功能如此强大，可以实现真正的容器间跨主机通信，那么我们该如何使用它呢？目前比较普遍的方法是结合 Open vSwitch 使用。前文简单介绍过 Open vSwitch 是一个功能强大的虚拟交换机，支持 GRE、VXLAN 等协议，因此 Open vSwitch 是一个不错的选择。

将 4.2.3 节中桥接方法的例子稍作修改，使两台主机处在不同的网络中，接着在两台主机中间建立 GRE 隧道，就可以使它上面的 Docker 容器进行通信，如图 4-12 所示。

图 4-12 中，两台 centos 的主机 hostl 和 host2， host1的IP为10.10.103.91/24，host2 的 IP为 10.10.105.235/24。为了解决两台主机上P地址冲突的问题，还是使用--fixed-cidr参数将不同主机上的 Docker 容器的地址限定在不同的范围中。ovs0 为 Open vSwitch 网桥，用来创建 GRE 隧道，并与 Docker 自带的网桥 docker0 桥接在-一起，如此一来，连接在 docker0 上的容器就可以通过 ovs0 的隧道到达另一台主机。具体操作如下：

在 host1 上做如下操作
配置--fixed-cidr 参数，重启 docker

```
echo 'DOCKER_ _OPTS="--fixed-cidr=172.17.1.1/24"' >> /etc/default/docker
service docker restart
```

创建 ovs0 网桥，并将ovs0 连在 docker0 上

```
ovs-vsctl add-br ovs0
brctl addif docker0 ovs0
```
在 ovs0 上创建 GRE 隧道
```
ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.105.235
```
在 host2 上做如下操作

```
echo 'DOCKER_ OPTS="--fixed-cidr=172.17.2.1/24"' >> /etc/default/docker
```

为避免和 host1. 上的 docker0 的 IP 冲突，修改 docker0 的 IP

```
ifconfig docker0 172.17.42.2/16
service docker restart
```
创建 ovs0 网桥，并将 ovs0 连接在 docker0 上

```
ovs-vsctl add-br ovs0
brctl addif docker0 ovs0
```
在 ovs0 上创建 GRE 隧道

```
ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ ip=10.10.103.91
```
创建 ovs0 网桥后，在主机上通过 ifconfig 命令可以看到一块名为 ovs0 的网卡。该网卡就是 ovs0 网桥自带的一个类型为 internal 的端口，就如同普通 Linux 网桥也有一个同名的端口一样，Linux 主机将其作为一块虚拟网卡使用。将这块网卡加入 docker0 后，就将 dockero 网桥和 ovs0 网桥级联了起来。

配置完成后，两台主机上的容器就可以通过 GRE 隧道通信了，下面来验证一下。

在 host1 上启动一个容器 con1
```
docker run -it --rm --name con1 centos /bin/bash
```
在 con1 容器中，操作如下
```
nc -1 172.17.1.1 9000
```
在 host2 上启动一个容器 con2
```
docker run -it --rm --name con1 centos /bin/bash
```
在 con2 容器中，操作如下
```
nc 172.17.1.1 9000
hi!
```
在 con1。上可以显示 con2. 上输入的内容，表示两台容器可以正常通信。

与 4.2.3 节中桥接方法一样，尽管不同主机上的容器 IP 有不同的范围，但它们还是属于同一个 子网 ( 172.17.0.0/16)。con1 向 con2 发送数据时，会发送 ARP 请求获取 con2 的 MAC 地址。ARP 请求会被 docker0 网桥洪泛到所有端口，包括和 ovs0 网桥相连的 ovs0 端口。ARP 请求到达 ovs0 网桥后，继续洪泛，通过 gre0 隧道端口到达 host2上的 ovs0 中，最后到达 con2。host1 和 host2 处在不同的网络中，该 ARP 请求是如何跨越中间网络到达 host2 的呢？ARP 请求经过 gre0 时，会首先加上一个 GRE 协议的头部，然后再加上一个源地址为 10.10.103.91、目的地址为 10.10.105.235 的 IP 协议的头部，再发送给 host2。这里 GRE 协议封装的是二层以太网帧，而非三层 IP 数据包。con1 获取到 con2 的 MAC 地址之后，就可以向它发送数据，发送数据包的流程和发送 ARP 请求的流程类似。只不过 docker0 和 ovs0 会学习到 con2 的 MAC 地址该从哪个端口发送出去，而无需洪泛到所有端口。

如果结合 4.2.4 节中 VLAN 划分的例子，还可以实现跨网络的 VLAN 划分。由于普通的 Linux 网桥并不支持 VLAN 功能，因此需要使用 pipework 直接将 Docker 容器连在 ovs0 网桥上，容器 IP 也由 pipework 指定。

4. GRE 实现 Docker 容器跨网络通信（容器在不同子网中）

在 4.2.3 节直接路由的方法中，不同主机上的容器是在不同的子网中，而不是在同一个子网中。使用 Open vSwitch 的隧道模式也可以实现此网络模型，如图 4-13 所示。

图中有两台 centos 的主机 host1 和 host2, host1 的IP地址为10.10.103.91/24，host2的IP地址为10.10.105.235/24。hostl 上的Docker容器在172.17.1.0/24子网中，host2上的容器在172.17.2.0/24子网中。由于两台主机不在同一个子网中，容器间通信不能再使用直接路由的方式，而需依赖 Open vSwitch 建立的 GRE 隧道进行，配置如下：

在 host1 上做如下操作

配置 docker0, 使Docker容器的IP在172.17.1.0/24 网络中
```
ifconfig docker0 172.17.1.254/24
service docker restart
```
创建 ovs0 网桥，并将 ovs0 连在 docker0 上
```
ovs-vsctl add-br ovs0
brctl addif docker0 ovs0
```
在 0 VS0 上创建一个 internal 类型的端口 rouO，并分配一个不引起冲突的私有 IP
```
ovS-vsctl add-port ovs0 rou0 -- set interface rou0 type=internal
ifconfig rou0 192.168.1.1/24
```
将通往 Docker 容器的流量路由到 rou0
```
route add -net 172.17.0.0/16 dev rou0
```
创建 GRE 隧道
```
ovs-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.103.91
```
删除 Docker 创建的 iptables 规则
```
iptables -t nat -D POSTROUTING -S 172.17.1.0/24! -0 docker0 -j MASQUERADE
```
创建自己的规则
```
iptables -t nat -A POSTROUTING -s 172.17.0.0/16 -o etho -j MASQUERADE
```
在 host2 上做如下操作

配置 dockero，使Docker容器的IP 在172.17.2.0/24 网络中
```
ifconfig docker0 172.17.2.254/24

service docker restart
```
创建 ovs0 网桥，并将 ovs0 连在 docker0 上
```
ovs-vsctl add-br ovs0
brctl addif docker0 ovs0
```
#在 ovs0 上创建一个 internal 类型的端口 rou0，并分配一个不引起冲突的私有 IP
```
ovs-vsctl add-port ovs0 rou0 -- set interface rou0 type=internal
ifconfig rou0 192.168.1.1/24
```
#将通往 Docker 容器的流量路由到 rou0
```
route add -net 172.17.0.0/16 dev rou0
```
#创建 GRE 隧道
```
oVS-vsctl add-port ovs0 gre0 -- set interface gre0 type=gre options:remote_ip=10.10.105.235
```
t 删除 Docker 创建的 iptables 规则
```
iptables -t nat -D POSTROUTING -S 172.17.2.0/24! -0 docker0 -j MASQUERADE
```
#创建自己的规则
```
iptables -t nat -A POSTROUTING -S 172.17.0.0/16 -0 etho -j MASQUERADE
```
在两台主机上分别创建一个 Docker 容器验证容器间的通信。

在 host1 上启动容器 con1
```
docker run -it --name con1 centos /bin/bash 
```
在 con1 容器中 
```
nc -1 9000
```
在 host2 上启动容器 con2
```
docker run -it --name con2 centos /bin/bash 
```
在 con2 容器中
```
nc -w 1 -v 172.17.1.1 9000

Connection to 172.17.1.1 9000 port  [tcp/*] succeeded!
```
本例中的网络模型与 Kubernetes 的网络模型类似，如图 4-14 所示。从网络的角度来看，此处一个容器可以视作 Kubernetes 的一个 pod，理解本模型有助于理解 K ubernetes 的网络”。


OpenVSwitch GRE/VxLAN隧道网络

● Kbr0 网桥代替了默认的 docker0 网桥

● Pod 间的流量通过 OVS 网桥在隧道中传输

● 图中的 NetworkX 可以是 LAN、Internet、EC2 VPC、SDN 等

● 隧道网络可以是静态的、基于流的或两者的结合


5. 多租户环境下的 GRE 网络

在多租户云环境下，租户之间的隔离显得非常重要。传统的 VLAN 方法有诸多限制，在大型公有云环境下并不合适。Overlay 网络在虚拟化场景下除了实现虚拟机间的跨网络通信外，还能填补 VLAN 的不足，满足多租户的隔离需求，OpenStack 的 Neutron 项目就是一个很好的实现例子。


OpenStack 是一个开源 IaaS 云平台，简而言之，就是能为用户提供虚拟机服务，如计算、存储和网络。Neutron 是 OpenStack 的一个子项目，提供虚拟环境下的网络功能。Neutron 有多种模式以满足不同的需求，其中 GRE 模式就是一个典型的多租户场景网络解决方案。鉴于 OpenStack 可以很好地管理虚拟机，并且有强大的网络功能，不妨学习一下 Neutron 的 GRE 模式，这对解决 Docker 网络问题有很好的启发作用。

Neutron 的 GRE 模式也是使用 Open vSwitch 来实现的，不同在于，Neutron 的 GRE 模式中有一个专门用来做 GRE 隧道的 Open vSwitch 网桥 br-tun，该网桥使用流表来转发数据包。流表是什么呢？其实，在 Open vSwitch 中，有两种工作方式- 普通模式和流表模式。

在普通模式下，Open vSwitch 交换机就如同一台普通的二层 MAC 地址自学习交换机一样，对于每一个收到帧，记录其源 MAC 地址和进入的端口，然后根据目的 MAC 地址转发到合适的端口，或者洪泛到所有的端口。

流表是 OpenFlow 引人的一个概念，OpenFlow 是一种新型的网络模型，可以用来实现 SDN  (Software Defined Network）。流表是由流表项组成的，每个流表项就是一个转发规则。每个流表项由匹配域、处理指令、优先级等组成。匹配域定义了流表的匹配规则，匹配域的匹配字段相当丰富，可以是二层 MAC 地址、三层 IP 地址、四层 TCP 端口等，而不仅仅是 MAC 地址。处理指令定义了当数据包满足匹配域的规则时，需要执行的动作。常见的处理动作包括将数据从某个端口转发出去、修改数据协议头部的某个字段、提交给其他流表等。每一个流表项属于一个特定的流表，并有一个给定的优先级。当有数据进入流表模式的 Open vSwitch 交换机后，首先会进入 tableO，按照流表项的优先级从高到低匹配。如果在 table 0 中没有匹配到相应规则则丢弃；如果匹配成功，则执行相应的动作。

Open vSwitch 的流表模式就是按照流表项转发数据的工作方式。如何判断 Open vSwitch 交换机使用的是哪种模式呢？可以使用 ovs-ofctl dump-flows  <bridgeName> 命令查看，如果仅有一条“NORMAL”的规则，则使用的是普通模式，如下所示。

```
ovs-ofctl dump-flows ovs0

cookie=0 x0, duration=93483.959 S, table=0, n_ packets=16, n_ bytes=1296, idle. _age=65534, hard_ age=65534, priority=0 actions=NORMAL
```
了解了 Open vSwitch 的流表模式之后，来分析 OpenStack 的 GRE 模式中，计算节点上网桥的连接和配置，如图 4-15 所示。


图 4-15 为一个 OpenStack 集群中两台计算节点之间的网络连接图”。每台主机上有两台虚拟机，vm1 和 vm3 属于租户 A，处在一个子网中；vm2 和 vm4 属于租户 B，处在另一个子网中。4 台虚拟机都连在 br-int 网桥上，且有 VLAN 标识，vm1 和 vm3 的 VLANID 为 1，vm2 和 vm4 的 VLAN ID 为 4。br-int 为一个工作在普通模式下的 Open vSwitch 交换机，它根据目的 MAC 地址和 VLAN ID 转发数据。和 br-int 相连的是 br-tun 网桥，br-tun 是隧道网桥，它根据流表转发数据。

通过 ovs-vsctl show 命令查看一下 host1。上两个网桥。上的端口信息，如下所示。
```
ovs-vsctl show

    Bridge br-int
        Port br-int
            Interface br-int
                type: internal

        Port patch-tun
            Interface patch-tun
            type: patch
            options: {peer=patch-int}

        Port "qvo7bc645a0-d7"
            tag: 1
            Interface "qvo7bc645a0-d7"

        Port "qvoec8a1d3e-dd"
            tag: 4
            Interface "qvoec8a1d3e-dd"

    Bridge br-tun
        Port br-tun
            Interface br-tun
            type: internal 
        
        Port patch-int
            Interface patch-int
            type: patch
            options: {peer=patch-tun}

        Port "gre-1"
            Interface "gre-1"
            type: gre
            options: {in_ key=flow, local_ ip="10.10.101.105", out_ key=flow,remote_ ip="10.10.101.110"}
```
其中，br-int 上标有“tag”信息的端口是虚拟机所连接的端口，类型为 veth pair。br-int 上的 patch-tun 端口和 br-tun上的 patch-int 相连，类型为 patch，功能和 veth pair 类似，用于连接两个 Open vSwitch 网桥。Open vSwitch 网桥上的每一个端口都有一个编号，可以使用 ovs-ofctl show  <bridgeName> 命令查看。端口的编号信息在流表的匹配规则中会使用到，这里先声明一下如何查看。

br-int 为普通模式下的网桥，并没有特别的流表规则。我们主要来关注一下 br-tun 网桥中的流表信息，可使用 ovs-ofctl dump-flows  <bridgeName> 命令查看，如下所示。

```
sudo ovs-ofctl dump-flows br -tun

NXST_ FLOW reply  (xid=0 x4):

table=0，。.. Priority=1, in port=1 actions=resubmit (,1)

table=0,... Priority=1, in_ port=2 actions=resubmit (,2)

table=0,... Priority=0 actions=drop

table=1,... Priority=1, dl_ _dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit (,20)

table=1,... Priority=1, dl_ dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit (,21)

table=2,... Priority=1, tun_ id=0 x3 actions=mod_ vlan_ vid:4, resubmit (,10)

table=2,... Priority=1, tun_ id=0 x2 actions=mod_ vlan_ vid:1, resubmit (,10)

table=2.... Priority=0 actions=drop

table=3,... Priority=0 actions=drop

table=10,... Priority=1

actions=learn (table=20, hard_ timeout=3 o0, priority=1, NXM_ 0 F. _VLAN_ _TCI [o..11], NXM. _0 F_ ETH. _DST [] =NXM_ 0 F_

ETH_ SRC [], load:0-> NXM _OF. _VLAN_ TCI [], load: NXM_ NX_ _TUN ID [] -> NXM_ NX_. TUN_ ID [], output: NXM_OF_ IN_

PORT []), output:1

table=20,... Priority=0 actions=resubmit (,21)

table=21,... Dl_ vlan=4 actions=strip_ vlan, set_ tunnel:0 x3, output:2

table=21,... Dl_ vlan=1 actions=strip_ vlan, set_ tunnel:0 x2, output:2

table=21,... Priority=0 actions=drop
```
以上流表规则的匹配流程可以用图 4- 16 来简单表示。



从图中可以看出，进入 br-tun 的流量有两条处理路径。其中-条处理从 patch-in 端口进入的流量，也即从本地虚拟机发送的流量；另一条处理从 GRE 隧道端口（如 gre-1) 进入的流量，也就是本地虚拟机接收的流量。

分析从虚拟机出去的流量的处理规则，此处涉及 table0、table1  、 table20 和 table21。

table0 有 3 条规则。第一条匹配从端口 1 进入的流量（in_ port=1)，匹配成功后提交给 table1 继续匹配（actions=resubmit (,1)），端口 1 就是 patch-int；第一条规则匹配从端口 2 进入的流量，匹配成功后提交给 table2 继续匹配，端口 2 就是 gre-1, 表示从 host2 中的虚拟机转发过来的流量。第三条规则将所有其他的流量丢弃（actions=drop）。

table1 处理的流量是从 patch-int 端口进人的，它有两条规则。一条定义了如果目的 MAC 地址是单播时 (dl_ dst=00:00:00:00:00:00/01:00:00:00:00:00），提交给 table20; 另一条定义了如果目的 MAC 地址是广播地址，则提交给 table21。

table20 处理的是单播流量。table20 本身只有一条规则，即将流量提交给 table21。事实上，table20 只有在没有记录转发包的目的 MAC 地址时才会交给 table21。table10 会通过从 GRE 端口进人的流量进行 MAC 地址自学习，将学习到的信息以流表的形式添加到 table20 中。这样一来，如果目标 MAC 地址已经被学习了，table20 则将数据包从合适的 GRE 端口转发出去“；如果没有被学习，则会提交给 table21，当作广播流量处理。

table21 处理的是广播流量和未匹配到目的 MAC 地址的单播流量，它有两条规则。一条匹配VLAN ID 为 1 的流量（dl_ vlan=1），也就是 VM1 发出的流量。匹配成功后，剥去 VLAN tag，将 GRE key 设为 2，从端口 2  (gre-1) 发送出去（actions=strip vlan, set_ tunnel:0x3, output:2），实际上是从所有的 GRE 端口发送出去。另一条规则匹配 VLAN ID 为 4 的流量，匹配成功后，剥去 VLAN tag，将 GRE key 设为 3, 从端口 2 发送出去。整体来说，table21 的处理流程就是做一个 VLAN ID 到 GRE key 的转换（VLAN 4 对应 GRE3; VLAN 1 对应 GRE2），然后将广播帧转发到所有 GRE 端口。

```
说明在 br-int 中，虚拟机使用 VLAN ID 来区分不同的租户，而通往外界的流量则使用 GRE key来区分。GRE key 有 24 位比特，可以很好地解决了 VLAN 数量不够用的问题。
```
接下来分析通往虚拟机的流量的处理规则，涉及 table0、table2 和 table10。

table2 处理的是从所有 GRE 端口进入的流量，它有 3 条规则。第一条匹配 GRE key 为 3 的流量（tun_ id=0x3)，匹配成功后，添加 VLAN tag，设置 VLAN ID 为 4，再提交给 tablel0  (actions= mod_ vlan_ vid:4, resubmit (,10））。第二条匹配 GRE key 为 2 的流量，匹配成功后，设置 VLAN ID 为 1，再提交给 tablel0。第三条丢弃所有不匹配的流量。table2 所做的动作正好和 table21 相反，将 GRE key 转换为 VLAN ID。

table10 处理从 table2 提交过来的流量，它只有一条学习规则。当有流量进来时，先从流量中学习源 MAC 地址、VLAN ID 等信息，并将规则添加到 table20 中。然后从端口 1 中转发出去，端口 1 即为 patch-int。

以上便是 host1 中 br-tun 流表处理规则，host2 中流表规则和 host1 类似。当从 VM1 ping VM3 时，VM1 先发出 ARP 请求，ARP 请求帧在 br-int 中被加上 VLAN tag, VLAN ID 为 1，再转发给 br-tun 网桥。在 br-tun 网桥中，ARP 请求帧被 table0 提交给 table1, table1 判断它是广播帧，提交给 table21, table21 再将其 VLAN tag 去掉，添加 GRE 协议头部，设置 GRE key 为 2, 然后从 GRE 端口发往 host2。ARP 请求帧到达 host2 中的 br-tun 网桥后，table2 将其 VLAN ID 置为 1，交给 tablel0。table10 学习到相关信息后，将其发往 host2 的 br-int 网桥。最终 VM3 收到 ARP 请求，并作出回应，告诉 VM1 自己的 MAC 地址。VM1 得知 VM3 的 MAC 地址后，会构造 ICMP 请求包发送给 VM3，数据处理路径和 ARP 响应的处理路径是-样的。该例子中，GRE 隧道中封装的是以太帧。

从 OpenStack 的 GRE 网络中，可以看到隧道技术实现了虚拟机之间的跨主机二层通信，并完成租户之间的隔离。使用 pipework、Open vSwitch 等工具，可以在 Docker 容器上模拟出如图 4-16 所示的场景，以实现同样的功能。其中难点在于 br-tun 流表的建立，但有了上面的示例和分析，使用者可以手动将需要的流表-条一条加进去，但需注意做好 VLAN ID 和 GRE key 之间的对应关系。如上例中，VLANID 1 对应 GREkey2, 代表租户 A; VLANID 4 对应 GREkey3, 代表租户 B。


下面介绍其中需要用到的一些关键命令的示例。

 (1) 使用 pipework 将 Docker 容器连接在 br-int 网桥上，并设置 VLAN。
```
ovS-vsctl add-br br-int
pipework br-int con1 192.168.100.1/24 @1
```
 (2) 使用 ovs -ofctl add-flow  <bridgeName> 命令向 Open vSwitch 中添加流表信息。
 ```   
ovs-ofctl add-flow br-tun "hard_ timeout=0 idle_ _timeout=0 priority=1 in_ port=1 actions=resubmit(,1) "
```
 (3) 在 Open vSwitch 上创建 GRE 隧道。
```
ovs-vsctl add-port br -tun gre0 -- set Interface gre0 type=gre options:local_ ip=192.168.100.100 options:in_ key=flow options:remote_ ip=192.168.100.101 options:out_ key=flow
```
