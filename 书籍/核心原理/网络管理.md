Docker 网络管理

虚拟化技术是云计算的主要推动技术，而相较于服务器虚拟化及存储虚拟化的不断突破和成熟，网络虚拟化似乎有些跟不上节奏，成为目前云计算发展的一大瓶颈。Docker 作为云计算领域的一颗耀眼新星，彻底释放了轻量级虚拟化的威力，使得计算资源的利用率提升到了一个新的层次，大有取代虚拟机的趋势。同时在前文提及，Docker 借助强大的镜像技术，让应用的分发、部署与管理变得异常便捷。那么，Docker 的网络功能又如何，能否满足各种场景的需求？本节就将介绍 Docker 网络的功能和实现方式。

#### 1 Docker 网络基础

在深人 Docker 内部的网络实现原理之前，先从一个用户的角度来直观感受一下 Docker 的网络架构与基本操作。

1. Docker 网络架构

Docker 在 1.9 版本中引人了一整套的 docker network 子命令和跨主机网络支持。这允许用户可以根据他们应用的拓扑结构创建虚拟网络并将容器接人其所对应的网络。其实，早在 Docker1.7 版本中，网络部分代码就已经被抽离并单独成为了 Docker 的网络库，即 libnetwork。在此之后，容器的网络模式也被抽象变成了统一接口的驱动。

为了标准化网络驱动的开发步骤和支持多种网络驱动，Docker 公司在 libnetwork 中使用了 CNM  (Container Network Model）。CNM 定义了构建容器虚拟化网络的模型，同时还提供了可以用于开发多种网络驱动的标准化接口和组件。

libnetwork 和 Docker daemon 及各个网络驱动的关系可以通过图 3-16 进行形象的表示。

如图 3-16 所示，Docker daemon 通过调用 libnetwork 对外提供的 API 完成网络的创建和管理等功能。libnetwork 中则使用了 CNM 来完成网络功能的提供。而 CNM 中主要有沙盒（sandbox）端点（endpoint）和网络（network）这 3 种组件。libnetwork 中内置的 5 种驱动则为 libnetwork 提供了不同类型的网络服务。下面分别对 CNM 中的 3 个核心组件和 libnetwork 中的 5 种内置驱动进行介绍。



图 3-16 Docker 网络虚拟化架构

CNM 中的 3 个核心组件如下。

❑ 沙盒：一个沙盒包含了一个容器网络栈的信息。沙盒可以对容器的接口、路由和 DNS 设置等进行管理。沙盒的实现可以是 Linux network namespace、FreeBSD Jail 或者类似的机制。一个沙盒可以有多个端点和多个网络。

❑ 端点：一个端点可以加入一个沙盒和一个网络。端点的实现可以是 veth pair、Open vSwitch 内部端口或者相似的设备。一个端点只可以属于一个网络并且只属于一个沙盒。

❑ 网络：一个网络是一组可以直接互相联通的端点。网络的实现可以是 Linux bridge、VLAN等。一个网络可以包含多个端点。

libnetwork 中的 5 种内置驱动如下。

❑ bridge 驱动。此驱动为 Docker 的默认设置，使用这个驱动的时候，libnetwork 将创建出来的 Docker 容器连接到 Docker 网桥上（Docker 网桥稍后会做介绍）。作为最常规的模式，bridge 模式已经可以满足 Docker 容器最基本的使用需求了。然而其与外界通信使用 NAT，增加了通信的复杂性，在复杂场景下使用会有诸多限制。

❑ host 驱动。使用这种驱动的时候，libnetwork 将不为 Docker 容器创建网络协议栈，即不会创建独立的 network namespace。Docker 容器中的进程处于宿主机的网络环境中，相当于 Docker 容器和宿主机共用同一个 network namespace，使用宿主机的网卡、IP 和端口等信息。但是，容器其他方面，如文件系统、进程列表等还是和宿主机隔离的。host 模式很好地解决了容器与外界通信的地址转换问题，可以直接使用宿主机的 IP 进行通信，不存在虚拟化网络带来的额外性能负担。但是 host 驱动也降低了容器与容器之间、容器与宿主机之间网络层面的隔离性，引起网络资源的竞争与冲突。因此可以认为 host 驱动适用于对于容器集群规模不大的场景。

❑ overlay 驱动。此驱动采用 IETF 标准的 VXLAN 方式，并且是 VXLAN 中被普遍认为最适合大规模的云计算虚拟化环境的 SDN controller 模式。在使用的过程中，需要一个额外的配置存储服务，例如 Consul、etcd 或 ZooKeeper。还需要在启动 Docker daemon 的的时候额外添加参数来指定所使用的配置存储服务地址。

❑ remote 驱动。这个驱动实际上并未做真正的网络服务实现，而是调用了用户自行实现的网络驱动插件，使 libnetwork 实现了驱动的可插件化，更好地满足了用户的多种需求。用户只要根据 libnetwork 提供的协议标准，实现其所要求的各个接口并向 Docker daemon 进行注册

❑ null 驱动。使用这种驱动的时候，Docker 容器拥有自己的 network namespace，但是并不为 Docker 容器进行任何网络配置。也就是说，这个 Docker 容器除了 network namespace 自带的 loopback 网卡外，没有其他任何网卡、IP、路由等信息，需要用户为 Docker 容器添加网卡、配置 IP 等。这种模式如果不进行特定的配置是无法正常使用的，但是优点也非常明显，它给了用户最大的自由度来自定义容器的网络环境。

在初步了解了 libnetwork 中各个组件和驱动后，为了帮助读者更加深入地理解 libnetwork 中的 CNM 模型和熟悉 docker network 子命令的使用，这里介绍一个 libnetwork 官方 GitHub 上示例的搭建过程，并在搭建成功后对其中容器之间的连通性进行验证，如图 3-17 所示。



图 3-17 CNM 主要组件示例图”

在这个例子中，使用 Docker 默认的 bridge 驱动进行演示。在此例中，会在 Docker 上组成一个网络拓扑的应用。

❑ 它有两个网络，其中 backend network 为后端网络，frontend network 则为前端网络，两个网络互不联通。

❑ 其中 containerl1 和 container3 各拥有一个端点，并且分别加入到后端网络和前端网络中。而container2 则有两个端点，图 3-17 它们两个分别加人到后端网络和前端网络中。

通过以下命令分别创建名为 backend 和 frontend 的两个网络。

```
docker network create backend
docker network create frontend
```

使用 docker network ls 可以查看这台主机上所有的 Docker 网络。

```
NETWORK ID NAME DRIVER

97 b529 e88 db9 backend bridge

c2 c8 c87 e975 f frontend bridge

fded32 C2349 a bridge bridge

93606 ac66 fdb none null

5 b4 c9 f6 ce4 d5 host host
```

除了刚才创建的 backend 和 frontend 之外，还有 3 个网络。这 3 个网络是 Docker daemon 默认创建的，分别使用了 3 种不同的驱动，而这 3 种驱动则对应了 Docker 原来的 3 种网络模式，这个在后面做详细讲解。需要注意的是，3 种内置的默认网络是无法使用 docker network rm 进行删除的。

在创建了所需要的两个网络之后，接下来创建 3 个容器，并使用如下命令将名为 container1 和 container2 的容器加人到 backend 网络中，将名为container3 的容器加人到 frontend 网络中。

```
docker run -it --name container1 --net backend busybox
docker run -it --name container2 --net backend busybox
docker run -it --name container3 --net frontend busybox
```
分别在 container1 和 container3 中使用 ping 命令测试其与 container2 的连通性，因为 container1 与 container2 都在 backend 网络中，所以两者可以连通。但是，因为 container3 和 container2 不在一个网络中，所以两个之间并不能连通。

可以在 container2 中使用命令 ifconfig 来查看此容器中的网卡及其配置情况。可以看到，此容器中只有一块以太网卡，其名称为 eth0, 并且配置了和网桥backend 同在一个 IP 段的 IP 地址，这个网卡就是 CNM 模型中的端点。

最后，使用如下命令将 container2 加入到 frontend 网络中。

```
docker network connect frontend container2
```

再次，在 container2 中使用命令 ifconfig 来查看此容器中的网卡及其配置情况。发现多了一块名为 eth1 的以太网卡，并且其 IP 和网桥 frontend 同在一个 IP 段。测试 container2 与 container3 的连通性后，可以发现两者已经连通。

可以看出，docker network connect 命令会在所连接的容器中创建新的网卡，以完成其与所指定网络的连接。

2. Bridge 驱动实现机制分析

前面我们演示了 bridge 驱动下的 CNM 使用方式，接下来本节将会分析 bridge 驱动的实现机制。

● dockero 网桥

当在一台未经特殊网络配置的 centos机器上安装完 Docker之后，在宿主机上通过使用 ifconfig命令可以看到多了一块名为dockero的网卡，假设IP 为172.17.0.1/16。有了这样一块网卡，宿主机也会在内核路由表上添加一条到达相应网络的静态路由，可通过 route -n 命令查看。

```
$ route -n

172.17.0.0 0.0.0.0 255.255.0.0 0 dockero
```

此条路由表示所有目的IP地址为172.17.0.0/16 的数据包从docker0 网卡发出。

然后使用 docker run 命令创建一个执行 shell ( /bin/bash）的 Docker 容器, 假设容器名称为 con1。

在 con1 容器中可以看到它有两块网卡 lo和 etho。lo设备不必多说，是容器的回环网卡；etho 即为容器与外界通信的网卡，etho 的IP 为172.17.0.2/16,和宿主机上的网桥docker0 在同一个网段。

查看 con1 的路由表，可以发现 con1 的默认网关正是宿主机的 docker0 网卡，通过测试，con1 可以顺利访问外网和宿主机网络，因此表明 con1 的 etho 网卡与宿主机的 docker0 网卡是相互连通的。

这时在其他控制台窗口查看宿主机的网络设备，会发现有一块以“veth”开头的网卡，如vethe043f86，我们可以大胆猜测这块网卡肯定是 veth 设备了，而 veth pair 总是成对出现的。在前面介绍过，veth pair 通常用来连接两个 network namespace，那么另一个应该是 Docker 容器 con1 中的 eth0 了。之前已经判断 con1 容器的 eth0 和宿主机的 docker0 是相连的，那么 vethe043f86 也应该是与 docker0 相连的，不难想到，docker0 就不只是一个简单的网卡设备了，而是一个网桥。

真实情况正是如此，图 3-18 即为 Docker 默认网络模式（bridge 模式）下的网络环境拓扑图，创建了 docker0 网桥，并以 veth pair 连接各容器的网络，容器中的数据通过 docker0 网桥转发到 etho 网卡上。



图 3-18 Docker 网络 bridge 模式示意图

这里网桥的概念等同于交换机，为连在其上的设备转发数据帧。网桥上的 veth 网卡设备相当于交换机上的端口，可以将多个容器或虚拟机连接在其上，这些端口工作在二层，所以是不需要配置 IP 信息的。图中 docker0 网桥就为连在其上的容器转发数据帧，使得同一台宿主机上的 Docker 容器之间可以相互通信。读者应该注意到 docker0 既然是二层设备，其上怎么也配置了 IP 呢？docker0 是普通的 Linux 网桥，它是可以在上面配置 IP 的，可以认为其内部有一个可以用于配置 IP 信息的网卡接口（如同每一个 Open vSwitch 网桥都有一个同名的内部接口一样）。在 Docker 的桥接网络模式中，docker0 的 IP 地址作为连于之上的容器的默认网关地址存在。

在 Linux 中，可以使用 brctl 命令查看和管理网桥（需要安装 bridge-utils 软件包）如查看本机上的 Linux 网桥以及其上的端口：

```
$ sudo brctl show

bridge name bridge id STP enabled interfaces

docker0 8000. 56847 afe9799 0 vethe043 f86
```

更多关于 brctl 命令的功能和使用方法，通过 man brct1 或 brctl --help 查阅。

Docker0 网桥是在 Docker daemon启动时自动创建的，其IP默认为172.17.0.1/16,之后创建的 Docker 容器都会在 docker0 子网的范围内选取一个未占用的 IP 使用，并连接到 docker0 网桥上。Docker 提供了如下参数可以帮助用户自定义 docker0 的设置。

❑ --bip=CIDR: 设置docker0的IP地址和子网范围，使用CIDR格式，如192.168.100.1/24。注意这个参数仅仅是配置 docker0 的，对其他自定义的网桥无效。并且在指定这个参数的时候，宿主机是不存在 docker0 的或者 docker0 已存在且 docker0 的 IP 和参数指定的 IP一致才行。

❑ --fixed-cidr=CIDR：限制 Docker 容器获取 IP 的范围。Docker 容器默认获取的 IP 范围为 Docker 网桥（dockero 网桥或者--bridge 指定的网桥）的整个子网范围，此参数可将其缩小到某个子网范围内，所以这个参数必须在 Docker 网桥的子网范围内。如 dockero的 IP为 172.17.0.1/16，可将--fixed-cidr设为172.17.1.1/24，那么Docker 容器的IP 范围将为 172.17.1.1~172.17.1.254。

❑ --mtu=BYTES：指定 docker0 的最大传输单元（MTU）。

除了使用 docker0 网桥外，还可以使用自己创建的网桥，使用--bridge=BRIDGE 参数指定。使用如下命令添加一个名为 bro 的网桥，并且为其配置 IP。

```
brctl addbr br0
ifconfig br0 188.18.0.1
```
然后在启动 Docker daemon 的时候使用--bridge=br0 指定使用 br0 网桥即可。注意此参数若和--bip 参数同时使用会产生冲突。

以上参数在 Docker daemon启动时指定，如 docker daemon --fixed-cidr=172.17.1.1/24。 在 centos中，也可以将这些参数写在 DOCKER_ OPTS变量中（位于/etc/defaultdocker 文件中），然后重启 Docker 服务。

● iptables 规则

Docker 安装完成后，将默认在宿主机系统上增加一些 iptables 规则，以用于 Docker 容器和容器之间以及和外界的通信，可以使用 iptables-save 命令查看。其中 nat 表上的 POSTROUTING链有这么一条规则:
```
-A POSTROUTING -s 172.17.0.0/16! -0 dockero -j MASQUERADE
```

这条规则关系着Docker容器和外界的通信，含义是将源地址为172.17.0.0/16 的数据包（即 Docker 容器发出的数据），当不是从 docker0 网卡发出时做 SNAT（源地址转换，将 IP 包的源地址替换为相应网卡的地址）。这样一来，从 Docker 容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到 Docker 容器的存在。那么，外界想要访问 Docker 容器的服务时该怎么办？我们启动一个简单的 Web 服务容器，观察 iptables 规则有何变化。

首先启动一个 Web 容器，将其 5000 端口映射到宿主机的 5000 端口上。

```
docker run -d -p 5000:5000 training/webapp python app. Py
```

然后查看 iptables 规则，省略部分无用信息。

```
$ sudo iptables-save

*nat

-A DOCKER ! -i docker0 -p tcp -m tcp --dport 5000 -j DNAT --to-destination 172.17.0.4:5000

*filter

A DOCKER -d 172.17.0.4/32! -i docker0 -0 docker0 -p tcp -m tcp --dport 5000 -j ACCEPT
```


可以看到，在 nat 和 filter 的 DOCKER 链中分别增加了一条规则，这两条规则将访问宿主机 5000 端口的流量转发到 172.1.0.4 的 5000 端口上（真正提供服务的 Docker 容器 IP 端口），所以外界访问 Docker 容器是通过 iptables 做 DNAT（目的地址转换）实现的。此外，Docker 的 forward 规则默认允许所有的外部 IP 访问容器，可以通过在 filter 的 DOCKER 链上添加规则来对外部的 IP 访问做出限制，如只允许源 IP 为 8.8.8.8 的数据包访问容器，需要添加如下规则：

```
iptables -I DOCKER -i dockero ! -s 8.8.8.8 -j DROP
```

不仅仅是与外界间通信，Docker 容器之间互相通信也受到 iptables 规则限制。通过前面的学习，了解到同一台宿主机上的 Docker 容器默认都连在 docker0 网桥上，它们属于一个子网，这是满足相互通信的第-步。同时，Docker daemon 会在 filter 的 FORWARD 链中增加一条 ACCEPT 的规则（--icc=true):

```
A FORWARD -i docker0 -o docker0 -j ACCEPT
```

这是满足相互通信的第二步。当 Docker daemon 启动参数--icc (icc 参数表示是否允许容器间相互通信）设置为 false 时，以上规则会被设置为 DROP, Docker 容器间的相互通信就被禁止，这种情况下，想让两个容器通信就需要在 docker run 时使用--link 选项。

在 Docker 容器和外界通信的过程中，还涉及了数据包在多个网卡间的转发（如从 docker0 网卡到宿主机 eth0 的转发），这需要内核将 ip- forward 功能打开，即将 ip_ forward 系统参数设为 1。Docker daemon 启动的时候默认会将其设为 1  (--ip-forward=true), 也可以通过以下命令手动设置：

```
echo 1 > /proc/sys/net/ipv4/ip_ forward

cat /proc/sys/net/ipv4/ip_ forward
```
以上过程中所涉及的 Docker daemon 启动参数如下。

❑ --iptables：是否允许 Docker daemon 设置宿主机的 iptables 规则，默认为 true。当设为 false 时，Docker daemon 将不会改变你宿主机上的 iptables 规则。

❑ --icc：是否允许 Docker 容器间相互通信，默认为 true。true 或 false 改变的是 FORWARD 链中相应 iptables 规则的策略（ACCEPT、DROP）。由于操作的是 iptables 规则，所以需要-- iptables=true 才能生效。

❑ --ip-forward：是否将 ip forward 参数设为 1, 默认为 true，用于打开 Linux 内核的 ip 数据包转发功能。

这些参数也是在 Docker daemon 启动时进行设置的，所以可以设置在 DOCKER_ OPTS 变量中。


● Docker 容器的 DNS 和主机名

同一个 Docker 镜像可以启动很多个 Docker 容器，通过查看，它们的主机名并不一样，也即是 说主机名并非是被写入镜像中的。在3.4节中已经提及，实际上容器中/etc目录下有3个文件是容 器启动后被虚拟文件覆盖掉的，分别是/etc/hostname、/etc/hosts、/etc/resolv.conf，通过在容器中运行 mount 命令可以查看。

```
$ mount

/dev/disk/by -uuid/1fec...ebdf on /etc/hostname type ext4. . .

/dev/disk/by. -uuid/1 fec...ebdf on /etc/hosts type ext4 . . .

/dev/disk/by-uuid/1fec...ebdf on /etc/resolv.conf type ext4...
```


这样能解决主机名的问题，同时也能让 DNS 及时更新（改变 resolv. Conf）。由于这些文件的维护方法随着 Docker 版本演进而不断变化，因此尽量不修改这些文件，而是通过 Docker 提供的参数进行相关设置，参数配置方式如下。

❑ -h HOSTNAME 或者--hostname=HOSTNAME: 设置容器的主机名，此名称会写在/etc/hostname和/etc/hosts文件中，也会在容器的bash 提示符中看到。但是在外部，容器的主机名是无法查看的，不会出现在其他容器的 hosts 文件中，即使使用 docker ps 命令也查看不到。此参数是 docker run 命令的参数，而非 Docker daemon 的启动参数。

❑ --dns=IP_ ADDRESS. . .：为容器配置DNS，写在/etc/resolv.conf 中。该参数既可以在 Docker daemon 启动的时候设置也可以在 docker run 时设置，默认为 8.8.8.8 和 8.8.4.4。

注意对以上 3 个文件的修改不会被 docker commit 保存，也就是不会保存在镜像中，重启容器也会导致修改失效。另外，在不稳定的网络环境下使用需要特别注意 DNS 的设置。

至此，Docker 基础的网络使用方式已经介绍完了，相信通过本节的介绍，读者对如何选择使用 libnetwork 的 5 种驱动已经有了一定的理解。下一节开始，将针对 Docker 网络配置的原理进行分析。



#### 2 传统的 link 原理解析

在使用 Docker 容器部署服务的时候，经常会遇到需要容器间交互的情况，如 Web 应用与数据库服务。了解到容器间的通信由 Docker daemon 的启动参数--icc 控制。很多情况下，为了保证容器以及主机的安全，--icc 通常设置为 false。这种情况下该如何解决容器间的通信呢？通过容器向外界进行端口映射的方式可以实现通信，但这种方式不够安全，因为提供服务的容器仅希望个别容器可以访问。除此之外，这种方式需要经过 NAT，效率也不高。这时候，就需要使用 Docker 的连接（linking）系统了。Docker 的连接系统可以在两个容器之间建立一个安全的通道，使得接收容器（如 Web 应用）可以通过通道得到源容器（如数据库服务）指定的相关信息。

在 Docker 1.9 版本后，网络操作独立成为一个命令组（docker network), link 系统也与原来不同了，Docker 为了保持对向上兼容，若容器使用默认的 bridge 模式网络，则会默认使用传统的 link 系统；而使用用户自定义的网络（user-defined network），则会使用新的 link 系统，这部分将会在下一小节介绍。

1. 使用 link 通信

link 是在容器创建的过程中通过--link 参数创建的。还是以 Web 应用与数据库为例来演示 link 的使用。首先，新建一个含有数据库服务的 Docker 容器，取名为 db。然后，新建一个包含 Web 应用的 Docker 容器，取名为 web，并将 web 连接到 db 上，操作如下。

```
docker run -d --name db training/postgres
docker run -d -P --name web --link db:webdb training/webapp python app.py
```
--link 参数的格式是这样的--link  <name or id>: alias。其中 name 是容器通过--name 参数指定或自动生成的名字，如“db”“web”等，而不是容器的主机名。alias 为容器的别名，如本例中的 webdb。

这样一个 link 就创建完成了，web 容器可以从 db 容器中获取数据。web 容器叫作接收容器或父容器，db 容器叫作源容器或子容器。一个接收容器可以设置多个源容器，一个源容器也可以有多个接收容器。那么，link 究竟做了什么呢? Docker将连接信息以下面两种方式保存在接收容器中。

❑ 设置接收容器的环境变量。

❑ 更新接收容器的/etc/hosts 文件。

2. 设置接收容器的环境变量

当两个容器通过--link 建立了连接后，会在接收容器中额外设置- -些环境变量，以保存源容器的一些信息。这些环境变量包含以下几个方面。

❑ 每有一个源容器，接收容器就会设置一个名为 <alias> _ NAME环境变量，alias为源容器的别名，如上面例子的web 容器中会有一个WEBDB_ NAME=/web/webdb 的环境变量。
  
❑ 预先在源容器中设置的部分环境变量同样会设置在接收容器的环境变量中，这些环境变量包括 Dockerfile 中使用 ENV 命令设置的，以及 docker run 命令中使用-e、--env=[]参数设置的。如 db 容器中若包含 doc=docker 的环境变量，则 web 容器的环境变量则包含 WEBDB_ ENV_ doc=docker。

❑ 接收容器同样会为源容器中暴露的端口设置环境变量。如 db 容器的 IP 为 172.17.0.2, 且暴露了 8000 的 tcp 端口，则在 web 容器中会看到如下环境变量。其中，前 4 个环境变量会为每

一个暴露的端口设置，而最后一个则是所有暴露端口中最小的一个端口的 URL（若最小的端口在 TCP 和 UDP 上都使用了，则 TCP 优先）。

```
WEBDB_PORT_8080_TCP_ADDR=172.17.0.82 
WEBDB_PORT_8080_TCP_PORT=8080 
WEBDB_PORT_8080_TCP_PROTO=tcp
WEBDB_PORT_8080_TCP=tcp: //172.17.0.82:8080
WEBDB_PORT=tcp: //172.17.0.82:8080
```

从上面的示例中，看到-link 是 docker run 命令的参数，也就是说 link 是在启动容器的过程中创建的。因此，回到容器的启动过程中，去看看 link是如何完成以上环境变量的设置的。我们发 现在容器启动过程中(daemon/start.go中的containerStart 函数）需要调用setuplinkedCon- tainers 函数，发现这个函数最终返回的是 env 变量，这个变量中包含了由于 link 操作，所需要额外为启动容器创建的所有环境变量，其执行过程如下。

 (1) 找到要启动容器的所有子容器，即所有连接到的源容器。

 (2) 遍历所有源容器，将 link 信息记录起来。

 (3) 将 link 相关的环境变量（包括当前容器和源容器的 IP、源容器的名称和别称、源容器中设置的环境变量以及源容器暴露的端口信息）放人到 env 中，最后将 env 变量返回。

 (4) 若以上过程中出现错误，则取消做过的修改。

值得注意的是，在传统的 link方式中，要求当前容器和所有的源容器都必须在默认网络中。

3.更新接收容器的/etc/hosts 文件

Docker 容器的 IP 地址是不固定的，容器重启后 IP 地址可能就和之前不同了。在有 link 关系的两个容器中，虽然接收方容器中包含有源容器 IP 的环境变量，但是如果源容器重启，接收方容器 中的环境变量不会自动更新。这些环境变量主要是为容器中的第一个进程所设置的，如sshd等守护进程。因此，link 操作除了在将link 信息保存在接收容器中之外，还在/etc/hosts 中添加了一项--源容器的 IP和别名 (--link参数中指定的别名），以用来解析源容器的IP地址。并且当源容器重启后，会自动更新接收容器的/etc/hosts 文件。需要注意的是这里仍然用的是别名，而不是源 容器的主机名 (实际上，主机名对外界是不可见的）。因此，可以用这个别名来配置应用程序，而不需要担心IP 的变化。

Docker 容器/etc/hosts 文件的设置也是在容器启动的时候完成的。在 3.8.2 节中介绍过 initializeNetworking 函数，在非 container 模式下，会调用这样一条函数链 allocateNetwork->ConnectToNetwork-> libnetwork. Controller. NewSandbox 来创建当前容器的 sandbox，在这个过程中会调用 setupResolutionFiles 来配置 hosts 文件和 DNS。配置 hosts 文件分为两步，一是调用 buildHostsFiles 函数构建当前 sandbox（对应当前容器）的 hosts 文件，先找到接收容器 (将要启 动的容器)的所有源容器，然后将源容器的别名和IP地址添加到接收容器的/etchosts 文件中；二是调用 updateParentHosts 来更新所有父 sandbox（也就是接收容器对应的 sandbox) 的 hosts文件， 将源容器的别名和IP地址添加到接收容器的/etc/hosts 文件中。

这样，当一个容器重启以后，自身的 hosts 文件和以自己为源容器的接收容器的 hosts 文件都会更新，保证了 link 系统的正常工作。

4. 建立 iptables规则进行通信

在接收容器上设置了环境变量和更改了/etc/hosts 文件之后，接收容器仅仅是得到了源容器的相关信息（环境变量、IP 地址），并不代表源容器和接收容器在网络上可以互相通信。当用户为了安全起见，将 Docker daemon 的--icc 参数设置为 false 时，容器间的通信就被禁止了。那么，Docker daemon 如何保证两个容器间的通信呢？答案是为连接的容器添加特定的 iptables 规则。

接着刚刚 web 和 db 的例子来具体解释，当源容器 (db容器) 想要为外界提供服务时，必定要 暴露一定的端口，如db容器就暴露了tcp/5432端口。这样，仅需要web容器和db容器在db容器的 tcp/5432端口.上进行通信就可以了，假如web 容器的IP地址为172.17.0.2/16，db容器的IP地址为 172.17.0.1/16，则web容器和db 容器建立连接后，在主机上会看到如下 iptables 规则。

```
-A DOCKER -s 172.17.0.2/32 -d 172.17.0.1/32 -i docker0 -0 dockero -p tcp -m tcp --dport 5432 -j ACCEPT

-A DOCKER -s 172.17.0.1/32 -d 172.17.0.2/32 -i dockero -0 dockero -p tcp -m tcp --sport 5432 -j ACCEPT
```

这两条规则确保了web容器和db 容器在db 容器的tcp/5432 端口上通信的流量不会被丢弃掉，从而保证了接收容器可以顺利地从源容器中获取想要的数据。

处理端口映射的过程是在启动容器阶段创建 endpoint 的过程中。仍然以 bridge 驱动为例，CreateEndpoint 最后会调用 allocatePort 来处理端口暴露。这里需要注意以下两点。

 (1) 得到源容器所有暴露出来的端口。注意这里是容器全部暴露的端口，而不仅仅是和主机做了映射的端口。

 (2) 遍历源容器暴露的端口，为每-一个端口添加如上的两条 iptables 规则。

Link是一种比端口映射更亲密的 Docker容器间通信方式，提供了更安全、高效的服务，通过环境变量和/etc/hosts 文件的设置提供了从别名到具体通信地址的发现，适合于一些需要各组件间通信的应用。

#### 3 新的 link 介绍

相比于传统的 link 系统提供的名字和别名的解析、容器间网络隔离（--icc=false）以及环境变量的注人，Dockerv1.9 后为用户自定义网络提供了 DNS 自动名字解析、同一个网络中容器间的隔离、可以动态加入或者退出多个网络、支持--link 为源容器设定别名等服务。在使用上，可以说除了环境变量的注人，新的网络模型给用户提供了更便捷和更自然的使用方式而不影响原有的使用习惯。

在新的网络模型中，link 系统只是在当前网络给源容器起了一个别名，并且这个别名只对接收容器有效。新旧 link 系统的另一个重要的区别是新的 link 系统在创建一个 link 时并不要求源容器已经创建或者启动。比如我们使用 bridge 驱动创建一个自定义网络 isolated_ nw，再运行一个容器 container1 加人该网络并链接另一个容器 contianer2，虽然 container2 还并不存在，如下代码所示。

```
$ docker network create isolated_nw

9 db097769 d0944234 c0427855 e6839167 b50 b97 dcbd9 ee31221 b7 b1 a463 b0617

$ docker run --net=isolated_nw -it --name=container1 --link container2:c2 centos

/#pingc2

ping: bad address 'c2'
```
可以看到，在上面的例子中，不需要依赖 container2 的存在而创建 link。下面再来创建 container2。
```
docker run --net=isolated_nw -itd --name=container2 centos

1 fb96362 c5 edd631 bafdef811606253 b530329 e80 cebdf79 ebeff44385 b0 b932
```
下面再来尝试网络的连通性。

```
/#ping c2

/etc # ping -c3 c2

PING c2  (172.18.0.3): 56 data bytes

64 bytes from 172.18.0.3: seq=0 tt1=64 time=0.441 ms

64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.196 ms

64 bytes from 172.18.0.3: seq=2 ttl=64 time=0.161 ms

--- C2 ping statistics ---

3 packets transmitted, 3 packets received, 0% packet loss

round-trip min/avg/max 。0.161/0.266/0.441 ms

/ # cat /etc/hosts

127.0.0.1 localhost

::1; localhost ip6- localhost ip6- loopback

fe0 o::0 ip6- localnet

ff0 o::0 ip6-mcastprefix

ff02::1 ip6-allnodes

ff02::2 ip6-allrouters

172.18.0.2 7 cc009 be825 a
```

可以看到，创建并启动完 container2 后, 就可以在 container1 里面 ping container2的别名 c2了（同样也可以ping名字和ID），验证了网络的连通性。另外在查看/etc/hosts 文件后，发现里面并没有 container2 的相关信息，这表示新的 link 系统的实现与原来的配置 hosts 文件的方式并不相同。实际上，Docker 是通过 DNS 解析的方式提供名字和别名的解析，这很好地解决了在传统 link 系统中由于容器重启造成注人的环境变量更新不及时的问题。在新的 link 系统下，用户甚至可以实现一对容器之间相互 link。


从本节的介绍中，可以看到 Docker 通过 libnetwork 库使用 Linux 网桥、端口映射、iptables 规则和 link 等技术完成了 Docker 的网络功能，已经能满足简单应用在单机环境下的基本需求。此外，libnetwork 还通过 overlay 驱动构建 overlay 网络允许跨主机通信；通过为用户创建独立的网络环境来实现多租户的隔离；通过配置 IPAM 也可以实现容器的固定 IP 等功能。目前，Docker 将网络单独成一个库-一 libnetwork，但是其还处于雏形阶段。在 Docker 官方完善网络功能之前，就需要引人额外的机制来扩展 Docker 的网络。本书第 4 章将继续介绍如何解决 Docker 用户的复杂网络需求。





