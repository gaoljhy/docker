Docker 存储管理

为了支持镜像分层、写时复制机制以及内容寻址存储（content-addressable storage）这些特性，Docker 设计了一套镜像元数据管理机制来管理镜像元数据。另外，为了能够让 Docker 容器适应不同平台不同应用场景对存储的要求，Docker 提供了各种基于不同文件系统实现的存储驱动来管理实际镜像文件。

#### 1 Docker 镜像元数据管理

Docker 镜像在设计上将镜像元数据与镜像文件的存储完全隔离开了。Docker 镜像管理相关的概念，包括 repository. Image、layer。Docker 在管理镜像层元数据时，采用的也正是从上至下 repository、Image、layer 三个层次。由于 Docker 以分层的形式存储镜像，所以 repository 与 image 这两类元数据并无物理上的镜像文件与之对应，而 layer 这种元数据则存在物理上的镜像层文件与之对应。下面将从实现的角度分条介绍 Docker 如何管理与存储这些概念。

1. Repository 元数据

repository 即由具有某个功能的 Docker 镜像的所有迭代版本构成的镜像库。 repository在本地的持久化文件存放于/var/lib/docker/image/some_graph_driver/repositories.json中，结构如下所示：

```
/var/lib/docker/image/aufs# cat repositories.json | python -mjson.tool
{
   "Repositories": {
        "busybox": {
            "busybox: latest":
                  "sha256:47bcc53f74dc94b1920f0b34f6036096526296767650f223433fe65c35f149eb"
        }
        "fedora": {
           "fedora: latest":
                   "sha256:ddd5c9c1dof2a08c5d53958a2590495d4f8a6166e2c1331380178af425ac9f3c"
        }
        "ubuntu": {
           "centos:7":
                   "sha256:90d5884b1ee07f7f791f51bab92933943c87357bcd2fa6be0e82c48411bb653"
        }
   }
}
```
文件中存储了所有 repository 的名字（如 busybox），每个 repository 下所有版本镜像的名字和 tag（如 busybox: latest）以及对应的镜像 ID。而 referenceStore 的作用便是解析不同格式的 repository名字，并管理 repository 与镜像 ID 的映射关系。

```
注意当前 Docker 默认采用 SHA256 算法根据镜像元数据配置文件计算出镜像 ID。
```

2. Image 元数据

image 元数据包括了镜像架构（如 amd64)、操作系统（如 Linux）、镜像默认配置、构建该镜像的容器 ID 和配置、创建时间、创建该镜像的 Docker 版本、构建镜像的历史信息以及 rootfs 组成。其中构建镜像的历史信息和 rootfs 组成部分除了具有描述镜像的作用外，还将镜像和构成该镜像的镜像层关联了起来。Docker 会根据历史信息和 rootfs 中的 diff_ ids 计算出构成该镜像的镜像层的存储索引 chainID，这也是 Docker 1.10 镜像存储中基于内容寻址的核心技术。

imageStore 则管理镜像 ID 与镜像元数据之间的映射关系以及元数据的持久化操作，持久化文件位于/var/lib/docker/image/[graph_driver]/imagedb/content/sha256/[image_id]中。

3. Layer 元数据

layer 对应镜像层的概念，在 Docker 1.10 版本以前，镜像通过一个 graph 结构管理，每一个镜像层都拥有元数据，记录了该层的构建信息以及父镜像层 ID，而最上面的镜像层会多记录一些信息作为整个镜像的元数据。graph 则根据镜像 ID（即最上层的镜像层 ID）和每个镜像层记录的父镜像层 ID 维护了一个树状的镜像层结构。

在 Docker 1.10 版本后，镜像元数据管理巨大改变之一便是简化了镜像层的元数据，镜像层只包含一个具体的镜像层文件包。用户在 Docker 宿主机上下载了某个镜像层之后，Docker 会在宿主机上基于镜像层文件包和 image 元数据，构建本地的 layer 元数据，包括 diff、parent、size 等。而当 Docker 将在宿主机上产生新的镜像层上传到 registry 时，与新镜像层相关的宿主机上的元数据也不会与镜像层一块打包上传。

Docker 中定义了 Layer 和 RWLayper 两种接口，分别用来定义只读层和可读写层的一些操作，又定义了 roLayer 和 mountedLayer，分别实现了上述两种接口。其中，roLayer 用于描述不可改变的镜像层，mountedLayer 用于描述可读写的容器层。

具体来说，roLayer 存储的内容主要有索引该镜像层的 chainID、该镜像层的校验码 diffID、父 镜像层 parent、graphdriver存储当前镜像层文件的 cacheID、该镜像层的大小 size 等内容。这些元数 据的持久化文件位于/var/lib/docker/image/[graph_driver]/layerdb/sha256/[chainID]/文件夹下，其中，diffID 和 size 可以通过一个该镜像层包计算出来；chainID 和父镜像层 parent 需要从所属 image 元数据中计算得到；而 cacheID 是在当前 Docker 宿主机上随机生成的一个 uuid，在当前宿主机上与该镜像层一一对应，用于标示并索引 graphdriver 中的镜像层文件。

在 layer 的所有属性中，difID 采用 SHA256 算法，基于镜像层文件包的内容计算得到。而ChainID 是基于内容存储的索引，它是根据当前层与所有祖先镜像层 dfID 计算出来的，具体算法如下。

❑ 如果该镜像层是最底层（没有父镜像层），该层的 diffID 便是 chainID。

❑ 该镜像层的 chainID 计算公式为 chainID(n)=SHA256(chain(n-1) diffID(n)），也就是根据父镜像层的 chainID 加上一个空格和当前层的 diffID，再计算 SHA256 校验码。

mountedLayer 存储的内容主要为索引某个容器的可读写层（也叫容器层）的 ID（也对应容器的 ID）、容器 init 层在 graphdriver 中的 ID--initID、读写层在 graphdriver 中的 ID--mountID 以及 容器层的父层镜像的 chainID--parent。 持久化文件位于/var/lib/docker/image/[graph_driver]/layerdb/mounts/[container_id]/路径下。

#### 2 Docker 存储驱动

为了支持镜像分层与写时复制机制这些特性，Docker 提供了存储驱动的接口。存储驱动根据操作系统底层的支持提供了针对某种文件系统的初始化操作以及对镜像层的增、删、改、查和差异比较等操作。目前存储系统的接口已经有 aufs、btrfs、devicemapper、vfs、overlay、zfs 这 6 种具体实现，其中 vfs 不支持写时复制，是为使用 volume  (Docker 提供的文件管理方式，docker数据卷介绍）提供的存储驱动，仅仅做了简单的文件挂载操作；剩下 5 种存储驱动支持写时复制，它们的实现有一定的相似之处。这里介绍 Docker 对所有存储驱动的管理方式，并以 aufs、overlay 和 devicemapper 为例介绍存储驱动的具体实现。

在启动 Docker 服务时使用 docker daemon -s some_ driver_ name 来指定使用的存储驱动，当然指定的驱动必须被底层操作系统支持。

1. 存储驱动的功能与管理

Docker 中管理文件系统的驱动为 graphdriver。其中定义了统一的接口对不同的文件系统进行管理，在 Docker daemon 启动时就会根据不同的文件系统选择合适的驱动，这里将针对 GraphDriver 中的功能进行详细的介绍。

● 存储驱动接口定义

GraphDriver 中主要定义了 Driver 和 ProtoDriver 两个接口，所有的存储驱动通过实现 Driver 接口提供相应的功能，而 ProtoDriver 接口则负责定义其中的基本功能。这些基本功能包括如下 8 种。

❑ String（）返回一个代表这个驱动的字符串，通常是这个驱动的名字。

❑ Create（）创建-一个新的镜像层，需要调用者传进一个唯一-的 ID 和所需的父镜像的 ID。

❑ Remove（）尝试根据指定的 ID 删除-一个层。

❑ Get（）返回指定 ID 的层的挂载点的绝对路径。

❑ Put（）释放一个层使用的资源，比如卸载-一个已经挂载的层。

❑ Exists（）查询指定的 ID 对应的层是否存在。

❑ Status（）返回这个驱动的状态，这个状态用一些键值对表示。

❑ Cleanup（）释放由这个驱动管理的所有资源，比如卸载所有的层。

而正常的 Driver 接口实现则通过包含一个 ProtoDriver 的匿名对象实现上述 8 个基本功能，除此之外，Driver 还定义了其他 4 个方法，用于对数据层之间的差异（diff）进行管理。

❑ Diff（）将指定 ID 的层相对父镜像层改动的文件打包并返回。

❑ Changes（）返回指定镜像层与父镜像层之间的差异列表。

❑ ApplyDiff（）从差异文件包中提取差异列表，并应用到指定 ID 的层与父镜像层，返回新镜像层的大小。

❑ DiffSize（）计算指定 ID 层与其父镜像层的差异，并返回差异相对于基础文件系统的大小。

GraphDriver 还提供了 naiveDiffDriver 结构，这个结构就包含了一个 ProtoDriver 对象并实现了 Driver 接口中与差异有关的方法，可以看作 Driver 接口的一个实现。

综上所述，Docker 中的任何存储驱动都需要实现上述 Driver 接口。当我们在 Docker 中添加一个新的存储驱动的时候，可以实现 Driver 的全部 12 个方法，或是实现 ProtoDriver 的 8 个方法再使用 naiveDiffDriver 进-一步封装。不管那种做法，只要集成了基本存储操作和差异操作的实现，一个存储驱动就算开发完成了。

● 存储驱动的创建过程

首先，前面提到的各类存储驱动都需要定义一个属于自己的初始化过程，并且在初始化过程中向 GraphDriver 注册自己。GraphDriver 维护了一个 drivers 列表，提供从驱动名到驱动初始化方法的映射，这用于将来根据驱动名称查找对应驱动的初始化方法。

而所谓的注册过程，则是存储驱动通过调用 GraphDriver 提供自己的名字和对应的初始化函数，这样 GraphDriver 就可以将驱动名和这个初始化方法保存到 drivers。

当需要创建一个存储驱动时（比如 aufs 的驱动）, GraphDriver 会根据名字从 drivers 中查找到这个驱动对应的初始化方法，然后调用这个初始化函数得到对应的 Driver 对象。这个创建过程如下所示。

 (1) 依次检查环境变量 DOCKER_DRIVER 和变量 DefaultDriver 是否提供了合法的驱动名字（比如 aufs），其中 DefaultDriver 是从 Docker daemon 启动时的--storage-driver 或者-s 参数中读出的。获知了驱动名称后，GraphDriver 就调用对应的初始化方法，创建一个对应的 Driver 对象实体。

 (2) 若环境变量和配置默认是空的，则 GraphDriver 会从驱动的优先级列表中查找一个可用的驱动。“可用”包含两个意思：第一，这个驱动曾经注册过自己；第二，这个驱动对应的文件系统被操作系统支持（这个支持性检查会在该驱动的初始化过程中执行）。在 Linux 平台下，目前优先级列表依次包含了这些驱动：aufs、btrfs、 zfs、devicemapper、overlay 和 vfs。

 (3) 如果在上述 6 种驱动中查找不到可用的，则 GrapthDriver 会查找所用注册过的驱动，找到第一个注册过的、可用的驱动并返回。不过这一设计只是为了将来的可扩展性而存在，用于查找自定义的存储驱动插件，现在有且仅有的上述 6 种驱动-一定会注册自己。

2. 常用存储驱动分析

了解了存储驱动的基本功能与管理方式以后，以 aufs、devicemapper 以及 overlay 为例，分析存储驱动的实现方式。

1. Aufs

首先，让我们来简单认识- -下 aufs。aufs  (advanced multi layered unification filesystem)“是一种支持联合挂载的文件系统，简单来说就是支持将不同目录挂载到同一个目录下，这些挂载操作对用户来说是透明的，用户在操作该目录时并不会觉得与其他目录有什么不同。这些目录的挂载是分层次的，通常来说最上层是可读写层，下层是只读层。所以，aufs 的每-层都是一个普通文件系统。

当需要读取一个文件 A 时，会从最顶层的读写层开始向下寻找，本层没有，则根据层之间的关系到下一层开始找，直到找到第一个文件 A 并打开它。

当需要写人一个文件 A 时，如果这个文件不存在，则在读写层新建一个；否则像上面的过程一样从顶层开始查找，直到找到最近的文件 A，aufs 会把这个文件复制到读写层进行修改。

由此可以看出，在第一次修改某个已有文件时，如果这个文件很大，即使只要修改几个字节，也会产生巨大的磁盘开销。

当需要删除一个文件时，如果这个文件仅仅存在于读写层中，则可以直接删除这个文件；否则就需要先删除它在读写层中的备份，再在读写层中创建一个 whiteout 文件来标志这个文件不存在，而不是真正删除底层的文件。

当新建一个文件时，如果这个文件在读写层存在对应的 whiteout 文件，则先将 whiteout 文件删除再新建。否则直接在读写层新建即可。

那么镜像文件在本地存放在哪里呢?

Docker的工作目录是/var/ib/docker,查看该目录下的内容可以看到如下文件。

```
/var/lib/docker# ls

aufs/ containers/ image/ network/ tmp/ trust/ volumes/
```

如果正在使用或者曾经使用过 aufs 作为存储驱动，就会在 Docker工作目录和 image下发现 aufs目录。image/aufs目录下的内容，是用于存储镜 像相关的元数据的，存储逻辑上的镜像和镜像层。

下面一起探究/var/lib/docker下另一个aufs文件夹。

```
/var/lib/docker/aufs# ls

diff/ layers/ mnt/
```

进入其中可以看到 3 个目录，其中 mnt 为 aufs 的挂载目录，diff 为实际的数据来源，包括只读层和可读写层，所有这些层最终一起被挂载在 mnt 上的目录，layers 下为与每层依赖有关的层描述文件。

最初，mnt 和 layers 都是空目录，文件数据都在 diff 目录下。一个 Docker 容器创建与启动的过程中，会在/var/ib/docker/aufs下面新建出对应的文件和目录。由于改版后，Docker 镜像管理部分与存储驱动在设计上完全分离了，镜像层或者容器层在存储驱动中拥有一个新的标示 ID，在镜像层（roLayer）中称为 cacheID，容器层（mountedLayer）中为 mountID。 在 Unix环境下，mountID 是随机生成的并保存在mountedLayer的元数据mountID中,持久化在image/aufs/layerdb/mounts/[container_ id]/mount-id 中。由于讲解的是容器创建过程中新创建的读写层，下面以 mountID 为例。创建一个新镜像层的步骤如下。

 (1) 分别在 mnt 和 diff 目录下创建与该层的 mountID 同名的子文件夹。

 (2) 在 layers 目录下创建与该层的 mountID 同名的文件，用来记录该层所依赖的所有的其他层。

 (3) 如果参数中的 parent 项不为空（这里由于是创建容器，parent 就是镜像的最上层），说明该层依赖于其他的层。GraphDriver 就需要将 parent 的 mountID 写人到该层在 layers 下对应 mountID 的文件里。然后 GraphDriver 还需要在 layers 目录下读取与上述 parent 同 mountID 的文件，将 parent 层的所有依赖层也复制到这个新创建层对应的层描述文件中, 这样这个文件才记录了该层的所有依赖。创建成功后，这个新创建的层的描述文件如下：

```
$ cat /var/lib/docker/aufs/layers/<mountID>/1 

父层的 ID

foce1c53a3d1ed981cf45c92c14711ec3a9929943c2e06128fb62281426c20b611 

接下来 3 条是父层的描述文件的全部内容

4fdd0019e2153bc182860fa260495e9cb468b8e7bbe1e0d564fd7750869f9095
40437055b94701b71abefb1e48b6ae585724533b64052f7d72face83fe3b95cd
ff3601714f3169317ed0563ff393f282fbb6ac9a5413d753b70da72881d74975
```

随后 GraphDriver 会将 diff 中属于容器镜像的所有层目录以只读方式挂载到 mnt 下，然后在 diff 中生成一个以当前容器对应的<mountID>-init命名的文件夹作为最后一层只读层，这个文件夹用于挂载并重新生成如下代码段所列的文件:

```
"/dev/pts": "dir",
"/dev/shm": "dir",
"/proc": "dir",
"/sys": "dir".
"/. Dockerinit": "file",
"/ . dockerenv": "file",
"/etc/resolv. conf": "file",
"/etc/hosts": "file",
"/ctc/hostnamc": "file",
"/dev/console" : "file",
"/etc/mtab": '/proc/mounts",
```

可以看到这些文件与这个容器内的环境息息相关，但并不适合被打包作为镜像的文件内容（毕竟文件里的内容是属于这个容器特有的），同时这些内容又不应该直接修改在宿主机文件上，所以 Docker 容器文件存储中设计了 mountID-init 这么一层单独处理这些文件。这一层只在容器启动时添加，并会根据系统环境和用户配置自动生成具体的内容（如 DNS 配置等），只有当这些文件在运行过程中被改动后并且 docker commit 了才会持久化这些变化，否则保存镜像时不会包含这一层的内容。

所以严格地说，Docker 容器的文件系统有 3 层：可读写层（将来被 commit 的内容）、init 层和只读层。但是这并不影响我们传统认识上可读写层+只读层组成的容器文件系统：因为 init 层对于用户来说是完全透明的。

接下来会在 diff 中生成-一个以容器对应 mountID 为名的可读写目录，也挂载到 mnt 目录下。所以，将来用户在容器中新建文件就会出现在 mnt 下以 mountID 为名的目录下，而该层对应的实际内容则保存在 diff 目录下。

至此我们需要明确，所有文件的实际内容均保存在 diff 目录下，包括可读写层也会以 mountID 命名出现在 diff 目录下，最终会整合到一起联合挂载到 mnt 目录下以 mountID 为名的文件夹下。接下来我们统一观察 mnt 对应的 mountID 下的变化。

首先让我们看看要运行的镜像对应的容器 ID，其容器短 ID为“7e7d365e363e“。

```
/var/lib/docker/aufs/mnt# docker ps -a

CONTAINER ID IMAGE COMMAND

7e7 d365 e363 e ubuntu:14.04 "/bin/bash"
```

查看容器层对应 mountID 为“7 e2152451105 f352a78421a9f78061bdc8 c9895002dcd12f7lbf49b 7057f2b45”。

```
/var/lib/docker/# cat image/aufs/layerdb/mounts/7e7d365e363e.../mount-id

7 e2152451105 f352 a78421 a9 f78061 bdc8 c9895002 dcd12 f71 bf49b7057f2b45
```

再来看看该容器运行前对应的 mnt目录，看到对应 mountID 文件夹下是空的。

```
/var/lib/docker/aufs/mnt# du -h. --max depth=1 | grep 7e2152451105f

4.0K ./7e2152451105 f352a78421a9f78061bdc8c9895002dcd12f71bf49b7057f2b45

4.0K ./7e2152451105 f352 a78421 a9 f78061 bdc8 c9895002 dcd12 f71bf49b7057f2b45- init
```

然后我们启动容器，再次查看对应的 mountID 文件夹的大小。
```
/var/lib/docker/aufs/mnt# docker start 7 e7 d365 e363 e

7 e7 d365 e363 e

/var/lib/docker/aufs/mnt# du -h. --max -depth=1 | grep 7e2152451105f

208M ./7e2152451105 f352a78421a9f78061bdc8c9895002dcd12f71bf49b7057f2b45

4.0K ./7e2152451105 f352 a78421 a9 f78061 bdc8 c9895002 dcd12 f71 bf49 b7057f2b45-init
```

可以看到以 mountID命名的文件夹变大了，进人可以看到已经挂载了对应的系统文件。

```
/var/lib/docker/aufs/mnt/7 e2152451105 f《此处省略部分》b45# ls -F

bin/ dev/ home/ lib64/ mnt/ proc/ run/ srv/ tmp/ var/Boot/ etc/ lib/ media/ opt/ root/ sbin/ sys/ usr/
```
接下来我们进入容器，查看容器状态，并添加一个 1 GB 左右的文件。

```
# docker exec -it 7 e7 d365 e363 e /bin/bash

root@7 e7 d365 e363 e: /# ls

bin dev home 1 ib64 mnt proc run SIV tmp var boot etc 1 ib media opt root sbin sys usr

root@7 e7 d365 e363 e: ~# mkdir test

root@7 e7 d365 e363 e: ~# cd test/

root@7 e7 d365 e363e: ~/test# ls

root@7 e7 d365 e363e: ~/test# dd if=/dev/zero of=test. Txt bs=1 M count=1024

1024+0 records in

1024+0 records out

1073741824 bytes  (1.1 GB) copied, 2.983 s, 360 MB/s

root@7 e7 d365e363e:~/test# du -h test. Txt

1.1 G test. Txt
```

当我们在容器外查看文件变化时可以看到，以 mountID 命名的文件夹大小出现了变化，如下所示。

```

/var/lib/docker/aufs/mnt# du -h. --max -depth=1 | grep 7 e2152451105f

1.3G ./7e2152451105f352a78421a9f78061bdc8c9895002dcd12f71bf49 b7057 f2b45

4.0K ./7e2152451105f352a78421a9f78061bdc8c9895002dcd12f71bf49 b7057 f2 b45- init
```


我们在容器中生成的文件出现在对应容器对应 mountID 文件夹中的 root 文件夹内。而当我们停止容器时，mnt'下相应 mountID 的目录被卸载，而 diff 下相应文件夹中的文件依然存在。当然，这仅限于当前宿主机，当我们需要迁移容器时，需要把这些内容保存成镜像再操作。

综上所述，以 aufs 为例的话，Docker 镜像的主要存储目录和作用可以通过图 3-13 来解释。



图 3-13 docker镜像在aufs文件系统的组织形式

最后，当我们用 docker commit 把容器提交成镜像后，就会在 diff 目录下生成一个新的 cacheID 命名的文件夹，存放了最新的差异变化文件，这时一个新的镜像层就诞生了。而原来的以 mountID 为名的文件夹已然存在，直至对应容器被删除。

2. Device Mapper

Device Mapper 是 Linux 2.6 内核中提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便地根据自己的需要制定实现存储资源的管理策略”。

简单来说，Device Mapper 包括 3 个概念：映射设备、映射表和目标设备，如图 3-14 所示。映射设备是内核向外提供的逻辑设备。一个映射设备通过一个映射表与多个目标设备映射起来，映射表包含了多个多元组，每个多元组记录了这个映射设备的起始地址、范围与一个目标设备的地址偏移量的映射关系。目标设备可以是一个物理设备，也可以是一个映射设备，这个映射设备可以继续向下迭代。一个映射设备最终通过- -棵映射树映射到物理设备上。Device Mapper 本质功能就是根据映射关系描述 IO 处理规则，当映射设备接收到 IO 请求的时候，这个 IO 请求会根据映射表逐级转发，直到这个请求最终传到最底层的物理设备上。


图 3-14 Device Mapper 机制示意图

Docker 下面的 devicemapper 存储驱动是使用 Device Mapper 的精简配置（thin-provisioning）和快照（snapshotting）功能实现镜像的分层。这个模块使用了两个块设备（一个用于存储数据，另一个用于存储元数据），并将其构建成一个资源池（thin pool）用以创建其他存储镜像的块设备。数据区为生成其他块设备提供资源，元信息存储了虚拟设备和物理设备的映射关系。Copy on Write 发生在块存储级别。devicemapper 在构建一个资源池后，会先创建一个有文件系统的基础设备，再通过从已有设备创建快照的方式创建新的设备，这些新创建的块设备在写人内容之前并不会分配资源。所有的容器层和镜像层都有自己的块设备，均是通过从其父镜像层创建快照的方式来创建（没有父镜像层的层从基础设备创建快照）。层次结构如图 3-15 所示。

值得说明的是，devicemapper 存储驱动根据使用的两个基础块设备是真正的块设备还是稀疏文件挂载的 loop 设备分为两种模式，前者称为 direct-lvm 模式，后者是 Docker 默认的 loop-lvm 模式。两种方式对于配置的好的 Docker 用户来说是完全透明的，驱动层的工作方式也一致，但由于底层存储方式不同导致两者性能差别很大。考虑到 loop-lvm 不需要额外配置的易用性，Docker 将其作为 devicemapper 的默认模式，但在生产环境中，推荐使用 direct-lvm 模式。

图 3-15 devicemapper 镜像层结构示意图 D

在已经创建好两个块设备的基础上，要使用 direct-lvm 模式的 devicemapper 存储驱动”，需要在 Docker daemon 启动的时候除了添加 -s=devicemapper 参数外，还要下列的参数指定存储数据和 元数据的块设备。

```
--storage -opt dm.datadev=/path/to/data

--storage-opt dm.metadatadev=/path/to/metadata与aufs一样，如果Docker使用过devicemapper存储驱动，在/var/ib/docker/ 下会创建 devicemapper/以及image/devicemapper目录。同样，image/devicemapper也是存储了镜像和逻辑镜像层的元数据信息。
```

最终，具体的文件都会存储在/var/ib/docker/devicemapper文件夹下，这个文件夹下有 3 个子文件夹，其中 mnt 为设备挂载目录，devicemapper 下存储了 loop-lvm 模式下的两个稀疏文件，metadata 下存储了每个块设备驱动层的元数据信息。

以 loop-lvm 模式为例，在 devicemapper 实际查看一下，可以看到 data 是一个 100 GB 的稀疏文件，它包含了所有镜像和容器的实际文件内容，是整个资源池的默认大小。每一个容器默认被限制在 10 GB 大小的卷内，可以通过重新启动 daemon，并添加参数--storage-opt dm.basesize=[size]调整基础块设备的大小，原有镜像层、容器层以及在原有镜像基础上创建的容器层的大小限制不受影响，只有在更改参数后 pull 下来的镜像的基础上创建的容器才会生效，并且 basesize 只能比原来的大，否则 daemon 会报错。

```
 [root@centos devicemapper] # 11 -h

total 519 M

-T------.1 root root 100 G 6 月 17 09:18 data

-T------.1 root root 2.0 G 6 月 17 09:18 metadata
```

可以看到，实际占用为 519 M，当我们再次 pul 新的镜像或者启动容器在其中增加文件时，基本上只增加了 data 文件的大小，其他文件并没有变化。

3. Overlay

OverlayFS 是一种新型联合文件系统（union filesystem），它允许用户将一个文件系统与另一个文件系统重叠（overlay），在上层的文件系统中记录更改，而下层的文件系统保持不变。相比于 aufs, OverlayFS 在设计上更简单，理论上性能更好，最重要的是，它已经进人 Linux 3.18 版本以后的内核主线，所以在 Docker 社区中很多人都将 OverlayFS 视为 aufs 的接班人。Docker 的 overlay 存储驱动便建立在 OverlayFS 的基础上。

OverlayFS 主要使用 4 类目录来完成工作，被联合挂载的两个目录 lower 和 upper，作为统一视图联合挂载点的 merged 目录，还有作为辅助功能的 work 目录。作为 upper 和 lower 被联合挂载的统一视图，当同一路径的两个文件分别存在两个目录中时，位于上层目录 upper 中的文件会屏蔽位于下层 lower 中的文件，如果是同路径的文件夹，那么下层目录中的文件和文件夹会被合并到上层。在对可读写的 OverlayFS 挂载目录中的文件进行读写删等操作的过程与挂载两层的 aufs（下层是只读层，上层是可读写层）是类似的。需要注意的一点是，第一次以 write 方式打开一个位于下层目录的文件时，OverlayFS 会执行一个 copy_up 将文件从下层复制到上层，与 aufs 不同的是，这个 copy_up 的实现不符合 POSIX 标准。OverlayFS 在使用上非常简单，首先使用命令 lsmod | grep overlay 确认内核中是否存在 overlay 模块，如果不存在，需要升级到 3.18 以上的内核版本，并使用 modprobe overlay 加载。然后再创建必要文件夹并执行 mount 命令即可完成挂载，最后可以通过查看 mount 命令的输出来确认挂载结果。

```
 [root@centos tmp] # mkdir lower upper work merged

 [root@centos tmp] # mount -t overlay overlay -olowerdir=./lower, upperdir=./upper,workdir=./work ./merged
 
 [root@centos tmp] # mount | grep overlay

overlay on /tmp/merged type overlay (rw,relatime,lowerdir=./lower, upperdir=./upper, workdir=./work)
```

在了解了 OverlayFS 的原理后，下面介绍一下 Docker 的 overlay存储驱动是如何实现的。

首先请读者直观感受一下overlay的目录结构。overlay存储驱动的工作目录是/var/ib/docker/ overlay/，在本书的实验环境中，该存储驱动下共存储了两个镜像与一个容器。
```
 [root@centos overlay] # tree -L 2

```
可以清楚地看到 overlay 目录下面以 UUID 命名的文件夹下的目录结构分为两种，一种是只有 root 目录的，另一种则有 3 个文件夹和一个文件 lower-id。根据 UUID 中是否带-init 后缀以及 UUID 名，很容易能判断出来，前者是镜像层的目录，后者是容器层（包括 ini 层）的目录。可能会觉得比较奇怪，为什么镜像层与容器层要采用不同的目录结构。前面介绍 OverlayFS 原理是将一层目录重叠于另一层目录之上，也就是说 OverlayFS 文件系统只会涉及两个目录，而 Docker 镜像却可能有许多层”。为了解决这种不对应的情况，overlay 存储驱动在存储镜像层的时候，会把父镜像层的内容“复制”到当前层，然后再写人当前层，为了节省存储空间，在“复制”的过程中，普通文件是采用硬链接的方式链接到父镜像层对应文件，其他类型的文件或文件夹则是按照原来的内容重新创建。所以上层镜像层拥有其依赖镜像层的所有文件，而最上面的镜像层则拥有了整个镜像的文件系统，这也是为什么镜像层对应的目录中只有一个 root 文件夹。

至于另一种目录结构，参照上面介绍 OverlayFS 工作的 4 种目录找到了对应关系。upper 对应上层目录，merged 对应挂载点目录，work 对应辅助工作（比如 copy_up 操作需要用到）目录，但 lower-id 却是一个文件，里面记录了该容器层所属容器的镜像最上面镜像层的 cache-id，在本书，上面的实验环境中，lower- id 内记录的是 e43c26d23b<省略部分。..> acca, Docker 使用该 cache-id 找到所依赖镜像层的 root 目录作为下层目录。

在准备最上层可读写容器层的时候，会将 init 层的 lower - id 与 upper 目录中的内容全部复制到容器层中。最后为容器准备 rootfs 时，将对应的 4 种文件夹联合挂载即可。下面通过在容器里面新建一个文件，然后在存储驱动对应目录中查看，具体的文件存储位置。

```
 [root@centos overlay] # docker exec ee010 c656c88 sh -C "echo 'Hello ZJU-SEL' > /root/SEL-TEST"

 [root@centos overlay] # cat 6b07e72 e9<省略部分。..>9 f87/merged/root/SEL-TEST

Hello ZJU-SEL

 [root@centos overlay] # cat 6b07e72 e9<省咯部分。..>9 f87/upper/root/SEL-TEST

Hello ZJU-SEL
```

最后需要说明一下，虽然 overlay 存储驱动曾经一度被提议提升为默认驱动，但其本身仍是一个发展相对初级的存储驱动，用户需要谨慎在生产环境中使用。相对于 aufs，除了本节开始提到的优点之外，由于 OverlayFS 只实现了 POSIX 标准的子集（例如 copy-up 等操作不符合 POSIX 标准），在运行在 overlay 存储驱动上的容器中直接执行 yum 命令会出现问题；另外一点就是，在使用 overlay 存储驱动时会消耗大量的 inode，尤其是对于本地镜像和容器比较多的用户，而 inode 只能在创建文件系统的时候指定”。

这里讨论了 Docker 对镜像元数据、文件系统的管理方法并介绍了 3 种典型存储驱动的具体实现。用户在使用 Docker 的时候，可以根据自己的需求和底层操作系统的支持情况灵活地选择最合适的存储驱动。





